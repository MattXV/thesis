\chapter{Conclusion}\label{ch:Conclusion}
The primary goal of this thesis has been to explore a novel system for acoustic rendering bespoke to \acrshort{ar} platforms, enabling context-aware rendering to consider characteristics of the environment surrounding the player. The objectives defined by this thesis developed and evaluated individual systems composing the pipeline proposed as a solution modularised into standalone contributions. Overall, the development of components evaluations conducted on test cases showed that the domain of acoustic rendering for \acrshort{ar} would benefit from applications of acoustic rendering pipelines, and the discussions on the results gathered open several potential avenues for future research heading towards more interactive and realistic auditory interactions.

\section{Summary of Contributions}
The following Sections will reflect on the objectives posed by the introductory chapter of this work (Section~\ref{sec:thesis-objectives}), highlighting significant results by connecting findings gathered from studies conducted throughout the development of the pipeline. For contextualisation, clarity, and accessibility, Table~\ref{tab:objectives-contributions} summarises the thesis aims and how Chapters contribute to each objective.
\input{objectives-contributions}

Overall, the significance of this thesis work lies in the proposal of a novel rendering pipeline, expressed by Chapter~\ref{ch:ar-pipeline}, which illustrates a design of the end-to-end system, targeting the primary objective, illustrating its concept and design principles, demonstrating implementations and apparatus design, and outlining the vision and potential expansions.\par
Chapter\ref{ch:Evaluation} evaluated a prototype deployed to an \acrshort{ar} platform, evaluating psychoacoustic factors in auditory displays, demonstrating the validity of the overarching aim by proving that a pipeline for realistic auditory displays has effects on interactions in virtual environments.\par


\subsection{Towards Integrating Computer Vision into Audio Augmented Reality}
As a high-level discussion around the interpreted results gathered from evaluating the system and its components, there are conclusions drawn that determine contributions towards the field of \acrfull{aar} and domains of computer graphics and vision in the context of simulating soundfields and generating realistic auditory stimuli. The following Sections will elaborate on the significance of the contributions, discussing their relevance to the broader research domains, target use cases, their limitations, and how future work can overcome them.

\begin{itemize}
    \item contributions in terms of methodologies presented
    \item novel testing techniques
    \item application to real-world examples 
    \item measuring user performance
\end{itemize}

\subsection{System Overview}
A prototype system for realistic auditory displays in \acrshort{ar}, considering basic features of the user's surrounding environment, has shown potential in evoking perceptual responses within applications and tasks in immersive experiences. Extrapolating from research trends in acoustic rendering domains, as shown in Chapter~\ref{ch:litReview}, deep learning-based real-time sound rendering is becoming increasingly accessible and feasible to deploy in \acrshort{xr} platforms. The modularity of the system proposed to address the thesis objective provides methodologies and evaluations that can apply to future extensions, facilitating the use of experimental sound rendering for wearable computing.

\subsection{Acoustic Characteristics Retrieval Methods}
The solutions proposed addressed the problem of extracting soundscape features in context-aware acoustic rendering target platforms equipped with space-sensing technology. The two proposed novel systems addressed the problem of retrieving acoustic characteristics from environments from their visual representations by adopting computer vision techniques. This thesis work and recent methods show that such a task is still an open research question due to the complexity in the mappings between visual representations of space and auditory stimuli generated from simulated soundfields based on properties assigned from inferring their visual appearance.\par
Mapping acoustic properties to representations of space has been defined in this work as the ``material tagging'' process, expressing the task engineers face when constructing immersive, complex scenes in game engines, where portions of scene geometry are assigned acoustic material properties. As acoustic simulations become more feasible for increasing use cases and applications, material tagging will incur higher manual authoring, which the application of computer vision techniques can automate, enabling and rendering procedural workflows.\par
There are benefits to the two proposed systems that aim at automating material tagging. With the camera-based acoustic material tagging system demonstrated in Section~\ref{sec:camera-tagging}, a novel system that uses camera renders to infer portions of scene geometry they depict, provides a solution that can adapt to cameras equipped by \acrshortpl{hmd}. It allows the system to scan and map acoustic characteristics to the environments as the user walks around their \acrshort{ar} scene.

The texture-based alternative system demonstrated in Section~\ref{sec:texture-tagging} expanded towards a more efficient workflow that also has applications to \acrshortpl{hmd} by using spatial reconstruction systems that provide and update scene geometry as the user navigates the scene.


\subsection{Acoustic Rendering}
Geometrical acoustics-based rendering pipelines have the advantage of controlling the number of rays and other aspects associated with the simulated acoustic phenomena. Thus, perceptual evaluations aimed at measuring psychoacoustic factors associated with auditory stimuli produced from simulated soundfields benefit from control over the resolution of the sound propagation technique. The degree of control is crucial for evaluating perceptual responses and defining the minimum requirements for psychoacoustic factors of the generated stimuli.\par

\subsection{Objective and Subjective Evaluations}
Despite the modularity of the proposed system and the standalone nature of individual components developed, evaluations and preliminary tests to evaluate their performance, efficacy, and human factors require consideration of the entire pipeline, including the final link of the chain, the listener. This final link is essential for determining perceptual responses and conducting ablation studies to evaluate the effect and the priority of individual components.\par
Of significant impact here are the methodologies in Chapters~\ref{ch:Materials} and~\ref{ch:acousticrendering}, where cutting-edge perceptual encoding techniques are leveraged to optimise preliminary testing of perceptual factors of systems for retrieving acoustic features from virtual environments. For instance, employing a deep perceptual audio similarity metric allows fast preliminary testing of the sound rendering pipeline, eliminating the need to recruit human subjects.

\begin{itemize}
    \item prototype limited to offline rendering
    \item sound rendering based on geometrical acoustics
    \item computer vision techniques applied to a variety of spaces
    \item computer vision techniques applied to a variety of sound renderers
    \item psychoacoustic testing has limited generalisability
\end{itemize}

\section{Limitations}
Despite the contributions achieved through this thesis work in the integration of sound rendering with computer vision, it is important to acknowledge the inherent limitations that need to be considered when interpreting findings. This section elaborates on specific limitations relative to the proposed system, and the experiments conducted range from the scalability of our algorithms in real-world applications to the generalisability of our results across different environments and scenarios. By openly discussing these aspects, we aim to provide a comprehensive understanding of the context within which our findings should be interpreted, thereby enriching the discourse and encouraging subsequent innovation in the field.

\subsection{Acoustic Material Tagging}
A limitation of the acoustic material tagging system arises from the methodology used for acoustic validation. Specifically, the testing protocols did not include comparisons against measurements obtained from impedance tubes, which are standard in evaluating the acoustic properties of materials. Impedance tubes offer precise data on how materials absorb and reflect sound, providing a critical benchmark for the accuracy of any sound rendering system. The absence of this comparison means our system's effectiveness in accurately rendering material-based acoustic responses might not fully align with real-world acoustic properties, potentially impacting the system's reliability in applications requiring high fidelity in sound simulation.\par
The two presented acoustic material tagging methods primarily utilised a single sound rendering method to evaluate the system's performance in recognising different materials. This limits the understanding of how the system might perform across the diverse spectrum of sound rendering techniques. Sound rendering methods, each with unique strengths and handling of acoustic phenomena, could significantly influence the accuracy and efficiency of material recognition required. Expanding the testing to include a broader range of sound rendering methods would provide a more comprehensive assessment of the system's versatility and applicability in various simulation contexts.\par
The choice of deep learning architecture plays a pivotal role in the system's ability to recognise materials based on their acoustic signatures. Exploring alternative deep learning models, like self-supervised or unsupervised approaches, could provide more suited systems for capturing the subtle nuances of surface characteristics, enhancing the system's overall accuracy and adaptability. 
The training dataset used for the two systems plays a critical role in the development of any machine learning system. The texture-based acoustic material tagging system was trained on the OpenSurfaces~\citep{bell2013opensurfaces} dataset, which, while extensive, does not encompass the full variety of materials encountered in real-world or high-fidelity virtual environments. This limitation could hinder the system's ability to generalise across unseen materials or unique surface characteristics not represented in the dataset. Enriching the training dataset with a broader and more diverse collection of surface types and material properties would likely enhance the system's robustness and accuracy, enabling it to better adapt to a wider array of virtual simulation scenarios.

\subsection{Rendering Pipelines for Augmented Acoustics}
Ga limitations

\subsection{Psychoacoustic Evaluation}
Limited space
GA system
More tasks
More psychoacoustic abilities


\section{Future Work}
While this research has taken steps towards integrating sound rendering with computer vision to enhance immersive experiences, it has also uncovered numerous pathways for future investigations. The evolving landscape of technology and the increasing need for multimodal applications in \acrshort{xr} domains present both challenges and opportunities for advancing the state of rendering domains. Future work should aim to address the limitations encountered in the current study, considering scalability, real-time processing demands, and the incorporation of more complex environmental interactions. Specific areas where future research could not only extend the findings of this study but also open new avenues for innovation in sound rendering and computer vision are outlined here, aiming at the next generation of immersive technologies.

\subsection{Visual-Acoustic Mappings}
\begin{itemize}
    \item An extended Image2Reverb that integrates listener and speaker positions in the mapping process.
\end{itemize}

Fast image-to-image network to manipulate existing IRs by using depth maps. Input IRs model the acoustic phenomena of the environment. The network learns how to manipulate input frequency-dependent IRs based on a depth map: e.g., an obstacle between emitter and receiver affects high-frequency reflections.

\subsection{Materials}
Future work can expand this work and feed into cross-domain applications by intersecting elements of the proposed pipeline with reinforcement learning and robotics. This thesis work could be applied to autonomous audio-visual navigation in 3D environments, where agents are trained to learn to perceive and respond to audio-visual entities with the goal of navigating to a specific target. This demonstrates that a robust approach to retrieving acoustic information from virtual environments can benefit audiovisual-based navigation applications. Generalisable and robust acoustic material systems have a direct impact on training models in differentiating and resolving features of soundscapes thanks to the complex mapping between visual features of surfaces in virtual environments and acoustic features in audio from simulated soundscapes.\par
Reinforcement learning models could also help improve 

\begin{itemize}
    \item A material optimiser based on ground truth RIRs and an agent tasked with painting surfaces with the correct materials.
    \item Unsupervised methods for material recognition using synthetic data to improve the workflow of expanding mappings between acoustic material databases and their appearance.
    \item Ablation study on material recognition pipelines
\end{itemize}

\subsection{Perceptual Response Analysis}
Chapter~\ref{ch:ch:acousticrendering} conducts extensive testing on the acoustic rendering pipeline employed by the \acrshort{ar} prototype and analyses objective and subjective factors of simulated soundfields. However, some unexplored avenues could expand the testing conducted, providing generalised guidelines for future sound rendering methods. A crucial expansion point here is the definition of a minimum input to a rendering pipeline to obtain a perceptually valid response --- an investigation that could be performed both on \acrshort{ga} and deep learning-based acoustic rendering systems.\par
Future work should analyse the minimum simulation resolution required for \acrshort{ga} pipelines, such as the number of rays or the order, as well as study the disparity between the reconstructed environment and the real space with respect to perceptual factors. The study would provide insights into the margins of error before the listener starts noticing discrepancies in perceived realism evoked by the simulated auditory display.\par


\subsection{Further Psychoacoustic Analysis}
The significance of the findings obtained from conducting the psychoacoustic study illustrated by Chapter~\ref{ch:Evaluation} has contributed towards creating guidelines for designing realistic interactions in \acrshort{ar}. When designing tasks in AR involving the application of psychoacoustic abilities, such as pinpointing the position of sound-emitting objects or resolving the nature of concurrent sound sources, acoustic modelling based on a virtual reconstruction of the space can suffice to affect task performance. From this practical implication stems the main theoretical contribution that transfers the established psychoacoustic characterisation of sound propagation effects to AR space, which has unlocked new potential in the realm of audio interactions. However, the theoretical contribution will require a set of complex scenes with diverse architectural and acoustic characteristics, evaluating the generalisability of the findings discussed in this work. Future work could benefit from the application of the testing methodology to a generalisable set of soundfields, including environments from high absorption and controlled acoustics, such as audio production rooms, to reverberant spaces like churches.\par

\section{Conclusive Remarks}
This thesis work builds on decades of advances toward systems for realistic auditory interactions for immersive media, adopting classic techniques for simulating soundfields and leveraging recent works within the domains of graphics and vision to optimise and automate several tasks associated with rendering believable auditory displays.\par
The primary goal of researching and proposing a novel system for \acrshort{ar} platforms was achieved, grounding its design choices on data collected from a series of experimental tests. Overall, the findings not only advance the understanding of sound rendering integrated with computer vision but also lay the groundwork for significant advances toward realistic auditory interactions in immersive virtual environments. This has implications not only for the entertainment industry, where enhanced immersive experiences can greatly elevate user engagement, but also for educational and training simulations, where realism can significantly improve learning outcomes and user task performance.\par
An important conclusive remark that might serve the reader food for thought is the unexplored potential that can be leveraged by combining machine vision with acoustic rendering applied to immersive platforms. Finally, the potential for these technologies to assist visually impaired individuals by providing more intuitive and context-aware navigational aids represents a meaningful step towards inclusive technology that bridges accessibility gaps. Perhaps the thesis work can serve as a stepping stone towards better multimodal immersive interactions and contribute to avenues identified throughout the manuscript.
