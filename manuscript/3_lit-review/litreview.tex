\chapter{Literature Review}\label{ch:litReview}% TODO Introduce each area properly
In light of the grounding provided around the domains of wave theory, digital signal processing, acoustics and soundfield simulations, as well as domains of computer graphics and vision, virtual environments, and immersive displays, this Chapter reviews the current state of research overlapping the primary aim of the thesis. Throughout this work, the thesis objectives may be reiterated to provide the reader with context relative to the Chapter or Section at hand. The overarching goal of this work is to explore the potential of computer vision within acoustic rendering pipelines for realistic sound transmissions between virtual sound-emitting objects perceived by a user in a virtual environment.\par
This Chapter gathers pioneering work, state-of-the-art, and experimental methods to address problems or tasks associated with each component of the thesis aim. Methods and experiments are reviewed, considering limitations and expansion points to inform design choices in engineering systems proposed throughout the following Chapters. Discussions around existing work aim at both orienting the reader towards the goal of each domain associated with the thesis component and defining the value of the contributions stemming from this work.\par

\section{Introduction}
The vision for augmented reality, ISMAR keynote.
The current state of interactive sound rendering
Emerging technology of space sensing
\cite{yang2022audio} audio interactions, removing unwanted noises from the augmented scene. Research towards multi-modal interactions has seen decades of advances INCLUDE REFS The field of Audio Augmented Reality (AAR) 
% Introduce virtual environments


\section{AAR: Audio Augmented Reality}
ref: Spatialised Audio Rendering for Immersive Virtual Environment]
The authors present a novel architecture for spatialised audio rendering for virtual environments experienced through immersive headsets. They define 3D sound localisation, room simulation, live audio input and efficiency as the main requirements the architecture should feature. The architecture draws from low-level rendering pipelines, such as graphic sub-systems, to integrate sound rendering procedures into existing scene-handling systems adopted by these pipelines. 


\section{Visual-Acoustic Mappings}\label{sec:lr-visual-acoustic-mapping}

Methods have been proposed to map visual representations of environments to their acoustic features. Sound rendering in virtual environments can leverage such mapping for producing audio stimuli conveying spatial information to the user. Recent work solves tasks within sound rendering for virtual environments, such as propagating audio within virtual environments. 
Spatial sound has been shown to be influential in having a significant effect on the sense of presence and immersion for a user in a VE \cite{poeschl13}. Factors of accurate and plausible acoustic rendering include geometry, material definitions and a room impulse response which describes the attenuation of sound from a sound source to a listener, and there exist approaches that tackle varying aspects of these factors.
A common denominator in the sound rendering methods mentioned is the problem of mapping visual representation of environments to corresponding acoustic materials, which intersects image processing and computer vision domains aimed at modelling how human vision recognise materials.\par%

\subsection{Modern Sound Rendering}
In Virtual Environments (VEs), acoustic rendering can reproduce spatial hearing abilities \cite{lokki2005navigation}, supporting architectural acoustics, cultural heritage \cite{berardi2016acoustic, vorlander2015virtual}, and computer games \cite{raghuvanshi2014parametric, mehra2015wave} to build compelling, realistic acoustic simulations. Recent advances in wavefield synthesis have made it easier and computationally feasible to apply to VEs~\cite{raghuvanshi2014parametric}. They draw on geometrical acoustics, wave-based or hybrid sound propagation algorithms, simulating sound propagation by tracing rays or beams \cite{hulusic2012acoustic}; solving the wave equations at discretised junctures of the representation of the environment or by a combination of the former techniques. They enable game designers to apply realistic, spatialised audio to games.

In acoustics, it is common to capture an environment adopting measurement techniques such as the sine sweep, usually consisting of reproducing a logarithm sine chirp or a short burst, e.g. a gunshot,  emulating a Dirac-delta function to excite frequencies in the audible spectrum and recording how the environment influenced the propagated sound at the listener position \cite{reilly1995convolution}. Such measurements can determine a Room Impulse Response (RIR), a series of reflection paths over time, recreating the acoustic space for a given source-listener position pair. Wave-based acoustic simulations achieve the highest degrees of realism in generating acoustic fields as they compute sound propagation via simulations of high-dimensional pressure fields \cite{raghuvanshi2014parametric} or solving the wave equation with Finite-Difference Time-Domain schemes \cite{hamilton2017fdtd}. Their inherently complex nature require solving the wave equation to produce acoustic simulations for a given scene, and despite recent GPU-based solvers optimising complexity by orders of magnitude \cite{mehra2012efficient}. Their computational requirements are often impractical for real-time applications due to the nature of the wave equation, resulting in numerical complexity increases with frequency. On the other end of the spectrum, Geometrical Acoustics (GA) provide methods for fast approximations of acoustic space; they have gained popularity among extended reality platforms due to their highly parallelisable implementations \cite{savioja2015overview}.\par
Schissler and Manocha \cite{schissler2016interactive} introduced an acoustic rendering system based on ray-tracing, adapting to large complex scenes. Among their contributions is overcoming the problem of handling many sound sources in large-scale environments by clustering them based on the distance from the listener. Based on an octree representation of space, with respect to the listener position, their clustering aggregates increasing numbers of sources as their distance from the listener increases. Their approach highlights the need for dissecting the acoustic space for efficient selective rendering, resulting in rendering of fine perceptual details within the listener's close proximity and coarse approximations otherwise. \par

Schissler \textit{et al.} \cite{schissler2021fast} recently presented a novel method for computing acoustic diffraction in real-time, which can adapt to GA frameworks. They target complex scenes typical of virtual and augmented reality applications. Their approach can overcome the inability of GA techniques to simulate phenomena intrinsic to waves.\par

\subsection{Experimental Methods for Sound Rendering}
Tang \textit{et al.} \cite{manocha2020differentiable} present a model for simulating sound fields using neural networks without pre-computing the wave field of an acoustic environment, predicting unseen objects with arbitrary shapes in a VE for sound propagation at interactive rates. They train a geometrical neural network on annotated meshes to infer acoustic data associated with the represented object.
\cite{chen2022visual} introduce a novel task dependant on this mapping, \textit{visual/acoustic matching}, which produces acoustic stimuli responding to a target space depicted in an image, given an input audio excerpt and an image of the environment in which excerpt propagated. The rapid development in DNN for multi-modal applications has opened new avenues in the field of sound propagation modelling, one of which tapped into visual-acoustic mapping, the process of determining relationships between visual and auditory features in audio-visual or immersive media.\par
One pioneering experiment,~\cite{Singh_2021_ICCV}'s Image2Reverb, towards this unexplored avenue used a DNN to define mappings between images and reverberation, expressed as an RIR. By observing photographs of real or virtual environments, our visual system is generally able to infer acoustic characteristics of the space; from a photograph of a cathedral, for instance, we can imagine its reverberant aural footprint. The authors aimed to leverage CNNs and GANs to explore automated mappings between deep visual features extracted from a given input image, representing an environment, and an output spectrogram of an RIR. Since many reverb metrics like the $T_{60}$ are linearly correlated to the energy decay in RIRs, their network encodes reverb by representing a spectrogram with variable energy decay. The authors train and evaluate the network by comparing results to ground truths pairs of photographs and measured responses, achieving around $0.87s$ mean error in $T_{60}$ estimations.\par
Improvements and new approaches to solving the task are being explored at increasing rates, such as~\cite{somayazulu2023self}'s network presenting a self-supervised visual-acoustic matching system. With an input audio excerpt and a target image representing an environment, their system re-synthesises the audio excerpt to reflect the acoustic features of the target environment. A key novel aspect of their method is the handling of reverberant audio by leveraging a state-of-the-art network for audio dereverberation (which is a well-established task in the field of acoustic signal processing). The de-reverberated audio is passed to a GAN, which optimises the output audio until it acoustically matches the extracted visual features.\par

\cite{liang2023neural} present a method that improves on the adoption of Neural Radiance Fields for sound propagation. The authors present a method that allows a neural field to be learned on a real soundscape by providing emitter and receiver position input, a ground truth RIR, and acoustic context representations. The neural field fit on the input environment allows the generation of novel RIRs based on given emitter-receiver position pairs. The neural field learns from multimodal representations of the environment, expressed as visual information by RGB + depth images and acoustic data by emitter-receiver position information.\par

\cite{tang2020scene} present a novel scene-aware sound rendering system aimed at rendering audio considering acoustic characteristics of a given room, providing real-time audio effects applied to novel signal matching the soundscape of the input room. The system uses a neural network to infer reverberation time and estimate resonance interferences caused by the room architecture using a recorded signal and 3D representation of the environment where the recording is generated. Their method uses the inferred acoustic properties as input to an acoustic simulator that generates and optimises acoustic materials by measuring simulation errors against the estimated room features. Once the acoustic simulator optimises materials, it convolves IRs with novel audio signals to emulate a sound source propagating in the input environment. As part of the testing procedures, the authors provide a benchmark for the material optimisation pipeline, outlining the error in estimated materials across rooms of increasing reverberation times; it increases with the size of the input room. Their system provides realistic acoustic stimuli as subjective tests show that the simulation error is not perceptually significant.\par

\cite{chen2023everywhere} use Audio-Visual receivers to sample reference features, generating joint audio-visual representations of input scenes to synthesise novel binaural audio. Their system takes visual information and uses a Joint Audio-Visual Representation to extract audio-visual features from space, which feed into an Integrated Rendering Head. The rendering head uses a ground-truth binaural waveform to optimise output binaural audio generated given a listener position. Their rendering pipeline improves state-of-the-art methods, such as few-shots learning-based techniques for sound rendering, by evaluating simulations on standard acoustic scenes and indoor space reconstruction datasets.\par

\cite{ratnarajah2022mesh2ir} present the first approach to neural networks for sound rendering.

Yang \textit{et al.} \cite{yang2020fast} present a method for synthesising Room Impulse Responses (RIRs), reproducing perceptually-convincing acoustics of real environments based on a small number of ultrasonic measurements.\par
Li \textit{et al.} \cite{li2018scene} identify a novel method for acoustic simulations using convolutional neural networks to perform acoustic analysis on videos, veering away from more formal 3D scene definition. This approach synthesises RIRs for environments' representations from audio-visual scenes. Their system extracts high-level acoustic properties such as reverberation time T\textsubscript{60} and frequency-dependent amplitude level (EQ).\par

\input{visual-acoustic-mapping}

\section{Material Recognition for Rendering Tasks}
\cite{schissler2017acoustic} present a two-stage system for sound rendering based on scene understanding performance on scans of physical space, requiring reconstruction of physical space and acoustic measurements as input and, leveraging recent advances in semantic segmentation for audio-visual rendering tasks. The first stage of the system uses multiple camera viewpoints to reconstruct a dense 3D triangle mesh representing the environment and generate input to a CNN to classify acoustic materials from camera renders. A Least Square Solver algorithm uses real measurements to optimise the inferred materials by calculating the distance from estimated IRs to the ground-truth IRs.\par
Semantic segmentation tasks aim to assign a semantic class label to every pixel in the input image. Examples of applications in scene understanding include PixelNet \cite{bansal2016pixelnet}, which performs semantic segmentation and edge detection; EdgeNet \cite{dourado2019edgenet}, which combines depth information with semantic scene completion, using RGB-D input data. For synthetic data generation, UnrealCV provides a pipeline that generates images from VEs providing semantic segmentations \cite{qiu2016unrealcv}, allowing for easy generation of training data.
Large-scale datasets, including semantic and 3D information, have been released, e.g. the Matterport3D dataset \cite{chang2017matterport3d}, which provides panoramic images generated across real environments. 
Various domain-specific applications of these methods have been proposed, e.g. in mixed and augmented reality \cite{chen2018context}, where semantic information about surfaces can guide contextual interactions between virtual elements and real-world structures; or surveillance \cite{mao2018aic2018}, where the semantics of objects in the scene determine its subsequent processing. 
However, few examples of applying computer vision to realistic audio rendering exist. One approach \cite{kim2019immersive} uses $360^{\circ}$ photographs and depth estimates to generate 3D geometry and semantic information, which is then used for physically-based audio rendering and can also adapt to VEs. In this context, even approximate semantic information could allow for gains in efficiency and a decrease in the costs of applying physically-based audio rendering to VEs.

Recent developments in deep learning techniques have contributed to a dramatic increase in accuracy in tasks such as image classification. Specifically, convolutional neural networks have been broadly adopted to learning functions mapping between image data and various semantic descriptors, such as local object classes \cite{long2015fully}, perceptual sensitivity \cite{Dolhasz_2020_CVPR}, or subjective quality \cite{bosse2017deep}. For example, Lagunas \textit{et al.} \cite{lagunas2019similarity}, present a method to learn similarities between materials based on their appearance and distinguish them in a feature space, informed by human perception. They describe mappings between subjective perception and physical material parameters. This is a challenging task due to the impact of low-level properties, such as illumination and reflectance on the appearance of materials. The authors address this problem using deep features learned by a neural network trained on a bespoke dataset, annotated with around about 100 classes of materials, captured under different conditions, including surface shape, illuminance and reflectance, expressed by environment maps and bidirectional reflectance distribution functions. In a subjective study they encode materials in a perceptually-informed feature space, allowing for calculation of perceptual distances.\par
Schwartz and Nishino \cite{schwartz2019recognizing}, address the problem of material recognition from local visual information of materials to better model human interaction. They aim to reduce manual supervision in the process of encoding material characteristics, explaining visual attributes such as shiny or metallic and material properties that may not be visually or locally discoverable such as softness. They present a novel method for material recognition consisting of perceptually-informed distances between materials and attribute spaces based on the distances.
Semi-supervised approaches have also been adopted in tackling such problems. For example, Gaur and Manjunath \cite{gaur2019superpixel} propose a novel deep learning architecture to cluster materials from a given dataset, improving state-of-the-art superpixel algorithms by combining segmentation of images into perceptually meaningful pixel clusters with a novel unsupervised clustering method based on superpixel embeddings. A novel loss function uses a variable-margin that compensates the limitations of classic superpixel algorithms in segmenting texture patterns, allowing the convolutional neural network to cluster superpixel labels based on their embeddings requiring no manual supervision or annotations.\par%

Approaches like these have demonstrated the efficacy of deep learning techniques for learning complex nonlinear mappings directly from annotated image data. This work leverages and adapts such techniques for the purpose of local material classification, allowing for the first step in mapping from image features to acoustic absorption coefficients.

\subsection{Findings and Limitations}
CNNs are becoming optimised enough that can be embedded in real-time systems. Generalising on surfaces is still an open problem in the research domain.
A crucial finding within this area derives from \cite{schissler2017acoustic}, who pioneered the field of scene understanding systems for sound rendering, projecting these into use cases for multi-modal AR and identifying limitations that future work should address to be around improved material recognition and inference on outdoor scenes. In general, the problem of recognising materials both in physical and virtual environments remains an open research question within these domains due to the challenging task of associating semantics to the visual appearance of surfaces in complex scenes, which depends on factors associated with the physical properties of surfaces or lighting conditions.  

\section{Perceptually-informed Rendering and Human Factors in Audio-Visual Rendering}
\cite{bonneel2010bimodal}'s pioneering study investigates the influence of audio-visual stimuli, as well as the interaction of graphics and audio, on material perception. They designed an experiment testing whether graphics and audio have significant effects on the subjective perception of material qualities. Their 

\cite{slater2009visual} Realistic rendering affects presence and interactions

\subsection{Findings and Limitations}
Cross-modal interactions within VEs are generated from representations of entities that are consumed by rendering algorithms to produce audio-visual stimuli. 


\section{Psychoacoustic Factors in Sound Rendering}
\cite{gonzalez2023binaural}

\subsection{Perception of Audio Quality}
\cite{rummukainen2018audio} pioneered the field of audio quality evaluation in immersive technology by porting MUSHRA-like testing to VR platforms, evaluating the impact of audio engines in interactive multi-modal VEs. The MUlti-Stimulus ranking test with Hidden Reference and Anchor (MUSHRA), described in the International Standard ITU-R BS.1534, is a standard approach for evaluating the perceived audio quality of a system, often employed to evaluate coding, compression or processing tasks in the audio domain \cite{series2014method}. Thanks to \cite{waet2015}'s web implementations, MUSHRA methods have been providing an essential tool for A/B comparisons or audio effects or algorithms and can be used to evaluate the quality of acoustic phenomena simulated with rendering techniques, such as reverberation, echo, diffraction or other soundscape characteristics that can be encoded in RIRs.\par
\cite{rummukainen2018audio}'s framework gives MUSHRA methods new dimensions in VR scenarios, enabling the evaluation of renderer or spatialiser systems such as HRTF spatialisers. The VR nature of the framework can provide a wide breadth of metrics associated with the interaction between the listener, sound-emitting entities, the environment and tasks or procedures. The user study the authors conducted demonstrates how these metrics can provide further insights into perceptive aspects, for instance, showing how participants dwelled around testing areas during the execution of the procedure. With modern HMDs providing more and better interaction and sensing technology, researchers have access to eye, head or hand-tracking data, as well as more information regarding scene elements of the VE. Such data is generally unexplored, and investigation should explore how acoustic renderers affect subjective responses to audio stimuli.\par

\subsection{Psychoacoustic Characterisation of Sound Propagation Methods}
With the human listener as the central and final link in the chain of a sound rendering system, it is essential to consider how the audio display presented to the listener is affected by aspects related to human perception and psychoacoustic abilities performed by the HAS. Due to the applications of sound rendering in VEs within serious and entertainment domains, researchers often base subjective evaluations of sound rendering techniques on task performance, studying how sound rendering techniques affect interactions, navigation, localisation or other activities influenced by the hearing sense. \par
\cite{mehra2015wave} presented a novel, wave-based sound rendering technique aimed at VR applications, advancing the domain of particle simulations for interactive sound rendering. One of the key contributions of their approach is providing a system offering realistic sound propagation between moving sound sources and listeners and can adapt to large, complex scenes. Their system offers spatial audio reproduction based on head tracking features of HMDs and position information of the listener in the VE. The inherent limitation of their approach is the required pre-computation stage for evaluating acoustic energy transfers between geometry and objects in order to solve particle equations and generate the wave propagation field that can then be solved at runtime using general-purpose GPUs. Here, the need for an evaluation of psychoacoustic factors arises for considering whether the perceived quality, subjective and psychoacoustic benefits outweigh the limitation of the pre-computation phase. They gathered 30 participants for their between-subjects experiment, 13 of whom had prior experience with VR technology, and the procedure they were asked to follow was the localisation of a sound-emitting object. The authors delegated a group for the navigation procedure using their renderer and a group using a geometrical acoustics renderer. They show that their wave-based sound renderer allowed a $27\%$ increase in localisation abilities in participants. Some of the limitations of this evaluation lie in the employment of an outdated image source-based renderer with edge diffraction as a comparison, altering the fairness of the study and the singular procedure used, as opposed to a range of different psychoacoustic-based activities that could be tested. \par
\cite{hacihabiboglu2017perceptual} Psychoacoustic factors in audio engine design

\subsection{Findings and Limitations}
There is a lack of research on the application of psychoacoustic abilities applied to tasks in XR environments.

\section{Conclusions}

The main conclusions of the chapter.
