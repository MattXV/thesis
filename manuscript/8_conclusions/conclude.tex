\chapter{Conclusion}\label{ch:Conclusion}
The primary goal of this thesis has been to explore a novel system for acoustic rendering bespoke to \acrshort{ar} platforms, enabling context-aware rendering to consider characteristics of the environment surrounding the player. The objectives defined by this thesis developed and evaluated individual systems composing the pipeline proposed as a solution modularised into standalone contributions. Overall, the development of components evaluations conducted on test cases showed that the domain of acoustic rendering for \acrshort{ar} would benefit from applications of acoustic rendering pipelines, and the discussions on the results gathered open several potential avenues for future research heading towards more interactive and realistic auditory interactions.

\section{Summary}
The following Sections will reflect on the objectives posed by the introductory chapter of this work (Section~\ref{sec:thesis-objectives}), highlighting significant results by connecting findings gathered from studies conducted throughout the development of the pipeline. For contextualisation, clarity, and accessibility, Table~\ref{tab:objectives-contributions} summarises the thesis aims and how Chapters contribute to each objective.
\input{objectives-contributions}

Overall, the significance of this thesis work lies in the proposal of a novel rendering pipeline, expressed by Chapter~\ref{ch:ar-pipeline}, which illustrates a design of the end-to-end system, targeting the primary objective, illustrating its concept and design principles, demonstrating implementations and apparatus design, and outlining the vision and potential expansions.\par
Chapter\ref{ch:Evaluation} evaluated a prototype deployed to an \acrshort{ar} platform, evaluating psychoacoustic factors in auditory displays, demonstrating the validity of the overarching aim by proving that a pipeline for realistic auditory displays has effects on interactions in virtual environments. 

\begin{itemize}
    \item objective of each chapter
    \item conclusions relating to each chapter
    \item key findings
\end{itemize}

\section{Towards Integrating Computer Vision into Audio Augmented Reality}
As a high-level discussion around the interpreted results gathered from evaluating the system and its components, there are conclusions drawn that determine contributions towards the field of \acrfull{aar} and domains of computer graphics and vision in the context of simulating soundfields and generating realistic auditory stimuli. The following Sections will break these down, discussing their relevance to the broader research domains, target use cases, as well as their limitations, and how future work has the potential to overcome them.

\begin{itemize}
    \item contributions in terms of methodologies presented
    \item novel testing techniques
    \item integrations of CDPAM into testing
    \item application to real-world examples 
    \item measuring user performance
\end{itemize}

\subsection{System Overview}
The primary contributions provided by the work on 
it uses geometrical acoustics-based acoustic rendering techniques. It should look at leveraging modern radiance field approaches.

\subsection{Acoustic Characteristics Retrieval Methods}
The solutions proposed addressed the problem of extracting soundscape features in context-aware acoustic rendering target platforms equipped with space-sensing technology. The two proposed novel systems addressed the problem of retrieving acoustic characteristics from environments from their visual representations by adopting computer vision techniques. This thesis work and recent methods show that such a task is still an open research question due to the complexity in the mappings between visual representations of space and auditory stimuli generated from simulated soundfields based on properties assigned from inferring their visual appearance.\par
Mapping acoustic properties to representations of space has been defined in this work as the ``material tagging'' process, expressing the task engineers face when constructing immersive, complex scenes in game engines, where portions of scene geometry are assigned acoustic material properties. As acoustic simulations become more feasible for increasing use cases and applications, material tagging will incur higher manual authoring, which the application of computer vision techniques can automate, enabling and rendering procedural workflows.\par
There are benefits to the two proposed systems that aim at automating material tagging. With the camera-based acoustic material tagging system demonstrated in Section~\ref{sec:camera-tagging}, a novel system that uses camera renders to infer portions of scene geometry they depict, provides a solution that can adapt to cameras equipped by \acrshortpl{hmd}. It allows the system to scan and map acoustic characteristics to the environments as the user walks around their \acrshort{ar} scene.

The texture-based alternative system demonstrated in Section~\ref{sec:texture-tagging} expanded towards a more efficient workflow that also has applications to \acrshortpl{hmd} by using spatial reconstruction systems that provide and update scene geometry as the user navigates the scene.


\subsection{Acoustic Rendering}
Geometrical acoustics-based rendering pipelines benefit from controlling the number of rays and other aspects associated with the simulated acoustic phenomena. Thus, perceptual evaluations aimed at measuring psychoacoustic factors associated with auditory stimuli produced from simulated soundfields benefit from control over the resolution of the sound propagation technique. The degree of control is crucial for evaluating perceptual responses and defining the minimum requirements for psychoacoustic factors of the generated stimuli.\par

\subsection{Psychoacoustic and Human Factors}


\begin{itemize}
    \item prototype limited to offline rendering
    \item sound rendering based on geometrical acoustics
    \item computer vision techniques applied to a variety of spaces
    \item computer vision techniques applied to a variety of sound renderers
    \item psychoacoustic testing has limited generalisability
\end{itemize}

\section{Future Work}
While this research has taken significant strides towards integrating sound rendering with computer vision to enhance immersive experiences, it has also uncovered a myriad of pathways for future investigations. The evolving landscape of technology and the increasing sophistication of auditory and visual simulations present both challenges and opportunities for deepening our understanding and capabilities in this domain. Future work should aim to address the limitations encountered in the current study, particularly focusing on scalability, real-time processing demands, and the incorporation of more complex environmental interactions. This Section outlines specific areas where future research could not only extend the findings of this study but also open new avenues for innovation in sound rendering and computer vision, aiming at the next generation of immersive technologies.

\subsection{Visual-Acoustic Mappings}
\begin{itemize}
    \item An extended Image2Reverb that integrates listener and speaker positions in the mapping process.
\end{itemize}

Fast image-to-image network to manipulate existing IRs by using depth maps. Input IRs model the acoustic phenomena of the environment. The network learns how to manipulate input frequency-dependent IRs based on a depth map: e.g., an obstacle between emitter and receiver affects high-frequency reflections.

\subsection{Materials}
Future work can expand this work and feed into cross-domain applications by intersecting elements of the proposed pipeline with reinforcement learning and robotics. This thesis work could be applied to autonomous audio-visual navigation in 3D environments, where agents are trained to learn to perceive and respond to audio-visual entities with the goal of navigating to a specific target. This demonstrates that a robust approach to retrieving acoustic information from virtual environments can benefit audiovisual-based navigation applications. Generalisable and robust acoustic material systems have a direct impact on training models in differentiating and resolving features of soundscapes thanks to the complex mapping between visual features of surfaces in virtual environments and acoustic features in audio from simulated soundscapes.\par
Reinforcement learning models could also help improve 


\begin{itemize}
    \item A material optimiser based on ground truth RIRs and an agent tasked with painting surfaces with the correct materials.
    \item Unsupervised methods for material recognition using synthetic data to improve the workflow of expanding mappings between acoustic material databases and their appearance.
    \item Ablation study on material recognition pipelines
\end{itemize}

\subsection{Perceptual Response Analysis}
Chapter~\ref{ch:ch:acousticrendering} conducts extensive testing on the acoustic rendering pipeline employed by the \acrshort{ar} prototype and analyses objective and subjective factors of simulated soundfields. However, some unexplored avenues could expand the testing conducted, providing generalised guidelines for future sound rendering methods. A crucial expansion point here is the definition of a minimum input to a rendering pipeline to obtain a perceptually valid response --- an investigation that could be performed both on \acrshort{ga} and deep learning-based acoustic rendering systems.\par
Future work should analyse the minimum simulation resolution required for \acrshort{ga} pipelines, such as the number of rays or the order, as well as investigates the disparity between the reconstructed environment and the real space with respect to perceptual factors. The study would provide insights into the margins of error before the listener starts noticing discrepancies in perceived realism evoked by the simulated auditory display.

\subsection{Further Psychoacoustic Analysis}
The significance of the findings obtained from conducting the psychoacoustic study illustrated by Chapter~\ref{ch:Evaluation} has contributed towards creating guidelines for designing realistic interactions in \acrshort{ar}. When designing tasks in AR involving the application of psychoacoustic abilities, such as pinpointing the position of sound-emitting objects or resolving the nature of concurrent sound sources, acoustic modelling based on a virtual reconstruction of the space can suffice to affect task performance. From this practical implication stems the main theoretical contribution that transfers the established psychoacoustic characterisation of sound propagation effects to AR space, which has unlocked new potential in the realm of audio interactions. However, the theoretical contribution will require a set of complex scenes with diverse architectural and acoustic characteristics, evaluating the generalisability of the findings discussed in this work. Future work could benefit from the application of the testing methodology to a generalisable set of soundfields, including environments from high absorption and controlled acoustics such as audio production rooms to reverberant spaces such as churches.\par


\section{Conclusive Remarks}
This thesis work builds on decades of advances toward systems for realistic auditory interactions for immersive media, adopting classic techniques for simulating soundfields and leveraging recent works within the domains of graphics and vision to optimise and automate several tasks associated with rendering believable auditory displays.\par
The primary goal of researching and proposing a novel system for \acrshort{ar} platforms was achieved, grounding its design choices on data collected from a series of experimental tests. Overall, the findings not only advance the understanding of sound rendering integrated with computer vision but also lay the groundwork for significant advances toward realistic auditory interactions in immersive virtual environments. This has implications not only for the entertainment industry, where enhanced immersive experiences can greatly elevate user engagement but also for educational and training simulations, where realism can significantly improve learning outcomes and user task performance.\par
An important conclusive remark that might serve the reader food for thought is the unexplored potential that be leveraged by combining machine vision with acoustic rendering applied to immersive platforms. Finally, the potential for these technologies to assist visually impaired individuals, by providing more intuitive and context-aware navigational aids, represents a meaningful step towards inclusive technology that bridges accessibility gaps. Perhaps the thesis work can serve as a stepping stone towards better multimodal immersive interactions and contribute to avenues identified throughout the manuscript.
