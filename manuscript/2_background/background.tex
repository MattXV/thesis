\chapter{Background}\label{ch:Background}

The following Sections introduce a body of knowledge from domains intersecting the overarching aim of this work, providing the reader with the necessary tools to dissect the components presented throughout the thesis. This Chapter begins by introducing basic sound physics, wave theory, and human perception as they define basic concepts of auditory interactions in the physical and virtual worlds.\par
Since a human listener is a link in the chain of the system proposed as part of this work, this Chapter will introduce basic psychoacoustic concepts linking the objective and technical aspects of sound rendering systems to human factors and subjective perception.

\section{Background on Human Hearing}
\subsection{Characteristics of the Human Hearing System}
The \acrfull{has} generally comprises two ears on either side of the human head, and each ear is a system that can be divided into three main parts: the outer ear, commonly referred to as the ear; the middle ear; and the inner ear. The outer ear, also referred to as the pinna, is shaped like a shell, is made of cartilage and skin, and serves both as protection for the system of receptors, ossicles, and nerves within the middle and the inner ear and a funnel that collects sound energy and transmits it to the middle ear via the outer ear canal. The outer ear canals are largely responsible for the frequency response of the \acrshort{has}. 
Auditory stimuli are sent to the brain via sensory cells that are surrounded by fluids displacing according to the received sound pressure. The middle ear converts sound pressure from the ear canal to displacement to these fluids, which send signals to the brain. This part of the ear is able to withstand variations of air pressure arriving at the outer ear and is responsible for matching different impedance magnitudes between the air or the medium in which sound is arriving at the apparatus and the impedance of the fluids in the inner ear.
The inner ear comprehends the vestibular system, a sensory system that allows humans to sense their spatial position, perceive rotation or displacement, and achieve balance and the cochlea. The cochlea, shaped like a snail, is embedded in the hard temporal bone, part of the skull \citep{zwicker2013psychoacoustics}.
Understanding basic functionalities of the \acrshort{has} is fundamental to reasoning auditory perception within virtual environments. When designing or evaluating sound rendering pipelines for immersive applications, factors influencing the perception of sound need to be considered.
Anthropometric features of humans affect how propagating sound waves are interpreted by listeners. Designing systems to simulate auditory perception in virtual environments requires considerations of key features of the \acrshort{has}. Section~\ref{sec:bg-psychoacoustics} will demonstrate how existing methods can simulate aspects of human listeners in digital systems.
Hearing, as a human ability, does not work in isolation and it is influenced by vision and other functions of human perception. The interaction of different senses, particularly how sound influences visual perception in virtual reality scenarios, affects how environments are perceived. The interaction of different senses, particularly how sound influences visual perception in virtual reality scenarios, affects how soundfields are perceived. Auditory stimuli can affect visual perception and vice-versa, adding dimensions of complexity to the process of designing an audio apparatus for immersive technology \citep{malpica2020crossmodal}.
The feature mismatch between visual and auditory cues can affect the externalization and perception of sounds. For instance, differences in quality between acoustic and visual stimuli can hinder the ability of a listener or observer to understand their surroundings from perceived stimuli, demonstrating the importance and validity of congruent sensory cues for effective auditory perception in virtual environments \citep{bonneel2010bimodal}.
With matching and coherent audio-visual stimuli, the human perception system can infer and ``fill in the blanks'', understanding spatial or semantic information of a sound-emitting object when visual information is scarce or missing. Replicating such ability in digital systems can be crucial for accessibility applications, allowing users with vision impairments to understand their surroundings by understanding acoustic characteristics.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{human_ear}
    \caption[Overview of the human hearing system]{Overview of the human hearing system \citep{zwicker2013psychoacoustics}}
    \label{fig:human_hearing_system}
\end{figure}

\subsection{Introduction to Psychoacoustics}\label{sec:bg-psychoacoustics}
The \acrshort{has} enables one of the fundamental functions of perception of surrounding space. In humans and species of the animal kingdom, hearing is the basis of many mechanisms, such as communication or survival instincts. Such mechanisms are neural processing applied to auditory stimuli arriving at the hearing system in order to compute tasks or solve problems, such as communicating using acoustical data or pinpointing the location of a sound-emitting entity relying on auditory stimuli. These are example applications or problems that can be solved by processing acoustical data interpreted by the \acrshort{has}.
Psychoacoustics investigates how the \acrshort{has} responds to auditory stimuli and investigates applications like loudness perception, localisation, lateralisation, or room volume estimation. The understanding of psychophysical responses of the \acrshort{has} to acoustical data in environments influences everyday activities that involve communicating, listening to musical instruments, or delivering messages to an audience of multiple listeners. The design process of built environments, infrastructure, or concert halls takes into account psychoacoustic factors to facilitate or improve human perception in specific environments during specific activities.
Psychoacoustic models significantly enhance the design and functionality of auditory displays by ensuring that sound reproduction systems are more aligned with the natural processing capabilities of the \acrshort{has}. Over the course of this thesis, several designs of audio rendering apparatuses will be discussed, comparing their effectiveness and efficiency. Psychoacoustic metrics provide an essential benchmark to test these systems, as they can indicate how effective is a method towards allowing users to perform natural abilities on a given stimulus. Studies such as~\cite{rungta2016psychoacoustic}'s indicate how well listeners can apply psychoacoustic abilities, like localisation or room volume estimation, on stimuli generated by experimental sound rendering pipelines.

\subsection{Sound Localisation}
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{loc_example}
        \caption{Sound localisation is the ability of the \acrshort{has} to pinpoint the position and direction of sound sources, red dots, at different directions and distances around the listener, centre.}
        \label{fig:loc-example}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{masking_example}
        \caption{Masking is a phenomenon that occurs when the presence of one sound makes it difficult or impossible to hear another sound at the same time.}
        \label{fig:masking-example}
    \end{subfigure}

       \caption[Visualisation of sound localisation and masking]{Visualisation of sound localisation and masking.}
       \label{fig:loc-masking-examples}
\end{figure}

Sound localisation is a natural ability of the \acrshort{has} that allows the determination of the direction and distance of a sound-emitting entity. This ability is essential for humans, animals, and autonomous agents, enabling sensing and discovering environments. In \acrshortpl{ve}, accurate sound localisation can greatly enhance immersion, making the experience more realistic and interactive by replicating a natural phenomenon in a virtual space.\par
The \acrshort{has} localises sound based on the acoustic cues that reach the ears; they can be binaural or monoaural. Binaural cues simultaneously involve the two ears (or receiving points), while monoaural cues consider mechanisms relating to individual receiving points. Binaural cues are the most significant for performing sound localisation as they consider differences in the arrival of sound at the two ears, revealing distance information between the source and each receiving point. These cues are often subdivided into \acrfull{itd} and \acrfull{ild}, referring to the differences in time and sound pressure level, respectively, at the time of arrival at the two receiving points.\par
Monoaural cues consider aspects relating to the physical and anatomical characteristics of the listener affecting propagating sound waves being perceived by the pinna, middle, and inner ear. Physical characteristics can include the shape of the pinna or the ear canal and other parts of the hearing system involved in the perception of sound pressure. These effects can transform or alter the spectrum of the perceived auditory stimuli \citep{blauert1997spatial, howard2013acoustics}.

\subsection{Psychoacoustics in Auditory Stimuli}
In the realm of digital audio technology, enabling listeners to apply psychoacoustic abilities to perceived auditory stimuli is an ongoing research area. Monoaural or two-channel audio formats, despite being among the most popular audio reproduction formats, have limited potential in expressing the directionality of sound sources and the sense of space around the entities existing in the audio scene being reproduced.\par
Ambisonics is a system engineered to overcome the issue and allow the reproduction of a surround image that expresses the direction, height, and distance of audio sources \citep{frank2015producing}.\par
Ambisonics is implemented through several formats; one of the most popular implementations, the B-Format uses spherical harmonics to represent the sound field. 
A special ambisonics microphone captures sound from all directions by adopting multiple capsules (often four) arranged to capture not just the intensity but also the direction of incoming sound waves. Sound is captured and reproduced using a set of audio channels that represent the sound field in terms of these spherical harmonics, which can describe sound coming from any direction around the listener. The audio from the microphone capsules is encoded into a multi-channel format. The basic form of ambisonics, known as first-order ambisonics, uses four channels: one for the overall sound pressure (omnidirectional) and three for directional information along the three spatial axes (X, Y, and Z) \citep{zotter2019ambisonics}.\par
Sound rendering pipelines often use \acrfullpl{hrtf} to model monoaural and binaural cues in virtual environments, to express directionality through stimuli more effectively and precisely. They describe how sound is affected by the listener's head, ears, and torso before reaching the ears. \acrshortpl{hrtf} are often individualised, i.e. modelled after an individual hearing system, and can be used to simulate how the listener perceives sounds from different directions. Modelling and creation of these functions often involve measurement systems to capture sound from varying distances and directions arriving at the hearing system \citep{zotkin2003hrtf}. However, sound localisation can also be affected by cognitive and psychological factors such as experience, expectation, and attention, as the brain can use past experiences and contextual information to make educated guesses about the location of sound sources.\par
In immersive applications, \acrshortpl{hrtf} have become fundamental in allowing sound localisation of virtual sound-emitting entities, though they can vary significantly between individuals, and creating a one-size-fits-all model for all listeners is still an open research question \citep{plugnplayAuralisations}.
There are research trends that focus on systems for individualising \acrshortpl{hrtf} automatically by predicting functions based on features of the target listener. Modern techniques make use of computer vision systems to infer \acrshort{hrtf} data from a virtual representation of a human head; i.e., a 3D scan of the listener's head and upper body \citep{zotkin2003hrtf}.

\subsection{Masking}
Masking occurs when the presence of one sound makes it difficult or impossible to hear another sound at the same time. This effect can significantly influence how sounds are perceived in everyday environments and is critical in the design of sound rendering pipelines for several reasons, including emulating how hearing systems operate or managing computational resources by avoiding rendering sounds that listeners cannot hear.
Masking can be classified into several types based on the characteristics of the sound responsible for masking other sounds (masker) or the sound being masked (maskee). Simultaneous Masking occurs when the masker and maskee are present at the same time. High-intensity frequencies can mask nearby lower-intensity frequencies, affecting the ability to discern sounds that are close in frequency range. Temporal Masking occurs when a masker precedes the maskee. Spectral Masking occurs when masking sounds across different frequency bands, where a strong presence in one band can affect the perception of sounds in another \citep{howard2013acoustics}.
Masking varies by individual and is influenced by cognitive factors, context, environment, and content or nature of the auditory stimuli; due to this, challenges remain in fully understanding and predicting masking effects.

\subsection{Just-Noticeable Differences}
The \acrfull{jnd} is the smallest change in a stimulus that can be detected by the sensory system. In the realm of hearing, it applies to various acoustic parameters, such as frequency (pitch), intensity (loudness), and duration (length of sound). The JND is not a fixed quantity but varies depending on the baseline intensity and frequency of the sound, as well as the listener's sensory acuity and environmental factors \citep{dolhasz2021perceptually}
Psychoacoustic models, which predict human auditory perception, incorporate \acrshortpl{jnd} to simulate how various sounds are processed and understood.  The concept of just-noticeable differences is vital for understanding human perception and forms the basis of numerous applications in psychoacoustics and to replicate phenomena of the \acrshort{has} in virtual environments. As research advances, our grasp of \acrshortpl{jnd} continues to refine the development and optimisation of sound rendering pipelines.

\section{Sound Propagation Background}
\subsection{Sound Propagation in Real and Virtual Environments}
Sound propagation is a transmission of energy in a sound field, which can be thought of as a superposition of sound waves travelling in a medium. In this work, we consider air as the sound propagation medium, which is assumed to be homogeneous, i.e., determining a constant velocity of sound $c$ expressed as: 
\begin{equation}
c = (331.4 + 0.6\Theta)~\frac{m}{s}   
\end{equation}
where $\Theta$ is the temperature in centigrade.
A vibrating object in a sound field causes air particles to move, initiating the transmission of energy in the field. Such an object is defined as a sound source, and if the intensity and frequency of the vibrations are within the perceptible range of the human hearing system, a listener may experience sound emitted by the said sound source.
In everyday sound transmissions, the air within sound fields is not at rest and features many inhomogeneities caused by external factors affecting the state of its particles, such as windows or air conditioning systems. However, according to \citep{kuttruff2016room}, such inhomogeneities are imperceptible, and generally, the air temperature has a perceptual effect on sound transmissions, especially in large concert halls and open spaces. Air temperature effects can be neglected in indoor sound propagation.

\subsection{Metrics and Descriptors of Real and Virtual Soundfields}
Standard acoustic parameters are crucial for measuring and evaluating soundfields, particularly in diverse environments like concert halls, studios, or public spaces. These parameters help in quantifying aspects such as sound quality, clarity, and diffusion, and they can express high-level characteristics of the behaviour of propagating sound waves in a given environment.\par
One of the most common acoustic parameters, reverberation time is essential for understanding one of the most characteristic aspects of an environment. It allows listeners to infer information about the size or basic architectural features of the space. $T_{60}$ reverberation time is defined as the time it takes for acoustic energy levels to drop by \qty{60}{\decibel} after a sound source has stopped emitting \citep{eckhardt1923acoustics}.\par
Similar to reverberation time but focused on the early part of the decay, providing a better description of the acoustic environment in terms of initial sound fading, Early Decay Time is another metric that provides insights into how energy behaves in environments. It is often calculated by fitting a curve on energy levels registered over time by a receiver \citep{jordan1970acoustical}.
The clarity index, also a commonly adopted metric, measures the clarity of sound in terms of its impact or sharpness.  It quantifies the ratio of early (within \qty{80}{\ms} or \qty{50}{\ms}) to late reflections \citep{reichardt1975definition}. Equation~\ref{eq:c80-def} shows how the ratio between early and late reflections is calculated: $\mathrm{p}$ denotes energy from a source, registered at the listener point at the time $\mathrm{t}$.
\begin{equation}\label{eq:c80-def}
    C_{80} = 10 \log \frac{\bigint_{0}^{80ms}\mathrm{p^2(t)d}t}{\bigint_{80ms}^{\infty}\mathrm{p^2(t)d}t}
\end{equation}
Similarly to clarity, the definition metric $D_{50}$ analyses early and late reflections, expressing sharpness and definition of propagating sound waves. The metric differs from clarity by quantifying the ratio between early reflections and the total aggregate reflections, see Equation~\ref{eq:d50-def}.
\begin{equation}\label{eq:d50-def}
    D_{50} = 10 \log \frac{\bigint_{0}^{50ms}\mathrm{p^2(t)d}t}{\bigint_{0}^{\infty}\mathrm{p^2(t)d}t}
\end{equation}
These metrics have strong relationships with subjective factors, like perceived quality or perceived resolution. As humans are often the target of digital systems that simulate soundfields, subjective factors need to be considered. This is crucial in the context of generating and validating acoustic simulations: although objective metrics can quantify and evaluate acoustic characteristics of soundfields, they require validation against human perception to assess psychoacoustic factors.\par
MUlti-Stimulus with Hidden Reference tests (MUSHRA) can be used to compare auditory stimuli subjectively. These tests are typically employed to assess digital signal processing algorithms, such as compression, and involve subjective testing. Subjects are often asked to rank a set of stimuli, expressing the perceptual distance between each stimulus and a reference.
Such testing methodologies can be adapted effectively to compare different soundfield reproductions, particularly in evaluating spatial audio and soundfield reproduction systems within immersive environments to assess perceived audio quality, as demonstrated by~\cite{rummukainen2018audio}.

\subsection{Digital Representation of Audiovisual Information}
\label{sec:DSP-background}
The following Sections will introduce background knowledge on Digital Signal Processing relevant to the representation of acoustic signals in digital systems and the manipulation of auditory stimuli in virtual environments. Digital Signal Processing methods and techniques provide building blocks for the construction of realistic 3D auditory displays in immersive technology.

\acrfull{dsp} is the science of analysing time-dependent physical processes. The acoustics realm deals with analogue signals and digital signals, terms used to indicate a continuous variation of amplitude values in a physical process. Electricity utilised to drive loudspeakers is an example of an analogue signal, expressing continuous changes in voltage applied to magnets to displace the position of a cone. The cone displacement causes pressure differences in air particles, transforming such changes in voltage to changes in air pressure, which the human auditory system interprets as sound. Acoustic signals consist of one or multiple sound waves oscillating, where each wave is an oscillation of energy at regular intervals; the duration of each interval determines the wavelength $\lambda$, and oscillations are measured as frequency in \unit{\hertz}. \par
On the other hand, a digital signal is a discrete representation of a continuous physical process, resulting in a sequence of measurement samples of an analogue signal expressed as amplitude values over time. Figure~\ref{fig:analogue-digital-signal} shows the difference between a continuous signal and a discrete signal: digital signal is represented with stems to indicate its nature of quantised measurements over time, abscissa, as opposed to a continuous change in amplitude, ordinate. The discrete nature of a digital signal has inherent problems and advantages that relate to the time interval between measurements: a digital signal representing an analogue one will always be an approximation of the continuous process as the system may change its state between measurement intervals. The approximated nature of digital signals causes information loss, which is counteracted by theories shown later, but allows digital systems to store and process acoustical data efficiently.\par
\acrshort{dsp} applies to both, but in this book chapter, we will only focus on the branch of \acrshort{dsp} that deals with digital signals. Digital systems like computers are used to process stored acoustical signals for several reasons, such as storing recordings of anechoic acoustic signals that simulation software can then process to generate realistic acoustic simulations, expressed as a processed digital signal. \par
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{analogue_digital}
    \caption[Analogue and digital signals]{Analogue and digital signal: the left axes show a continuous signal and the right axes show a discrete sampled representation.}
    \label{fig:analogue-digital-signal}
\end{figure}

The process of converting continuous signals to digital information involves taking measurements of the amplitude of a continuous signal at regular time intervals. There are two dimensions in which the analogue signal is measured during this process, the amplitude and time, respectively, the abscissa and the ordinate of Figure~\ref{fig:analogue-digital-signal}. Due to the physical limitations of digital technology, A/D converters can only take a finite number of measurements between time intervals, and they have limited accuracy in representing amplitude levels. In Figure \ref{fig:analogue-digital-signal}-b, it is possible to see how an A/D converter sees analogue signals: given an acoustic continuous signal as input, it takes amplitude samples at every time step, marked with by red ticks, and measures using the available amplitude levels (the dotted horizontal lines). As a result, the process outputs a series of data points, the red dots, approximating the input, and the resolution and fidelity of the approximation depend on the time elapsed between time steps and the available amplitude level points. There are standards to ensure the reproduction and manipulation of acoustical signals in digital systems with an appropriate fidelity, such as the ``Red Book'' IEC 60908 standard, adopted for the Compact Disc music format, determining that digital signals must be represented by 44.100 measurement samples per second, at 16bit amplitude resolution. 16-bit refers to the binary representation adopted by digital systems to store amplitude values, allowing $2\textsuperscript{16}=65,535$ possible amplitude levels. The sampling frequency, the number of measurements per second, is calculated in Hertz (Hz), and it is a fundamental property of digital signals that must be taken into account for almost all types of audio manipulation and analysis involved in acoustical applications and it paramount to correct reconstructions of any acoustic information in any digital system. \par
The Nyquist-Shannon sampling theorem is used to ensure a digital system reconstructs an analogue signal correctly. The theorem proves that a wave must be sampled at least twice during each oscillation period. A periodic wave oscillating at $20kHz$, which is around the maximum perceivable frequency in the human hearing range, would need to be sampled at least $40,000$ per second; hence, the standard $44.1kHz$ sampling rate. In Figure~\ref{fig:aliasing}, for instance, a \qty{50}{\hertz} signal is sampled at  \qty{90}{\hertz}, below the  \qty{100}{\hertz} Nyquist sampling frequency, causing aliasing, an incorrectly reconstructed signal that will be able to oscillate at a maximum frequency of  \qty{45}{\hertz}.\par
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{aliasing}
    \caption[Digital reconstruction of analogue signal]{The blue signal is being sampled at a sampling frequency lower than the Nyquist frequency, causing aliasing, an incorrect reconstruction of the digital signal, as opposed to Figure~\ref{fig:analogue-digital-signal} that shows a correctly reconstructed signal. As a result, the dotted green signal is created instead, having a frequency between \qty{0}{\hertz} and the sampling frequency.}
\label{fig:aliasing}
\end{figure}

\subsubsection{Analysis of Digital Signals}
Acoustical signals are often analysed in the time domain, as varying sound pressure levels over time, or in the frequency domain. By considering acoustical signals as a Fourier series, a function composed of sine or cosine primitives, the frequency domain representation determines how the power of an acoustical signal is distributed in a range of sine and cosine functions with wavelengths usually ranging from the minimum to the maximum perceivable frequencies of the human auditory system --- low to high frequencies. Time- and frequency-domain representations are often used for both analysis and manipulation of acoustical signals, often adopted in tasks like determining the effects of an environment in the perception of sound emitted by an object and arriving to a listener in said environment~\cite{ballou2013handbook}.\par 
In acoustics for interactive applications, engineers often adopt the \acrfull{dtft}, a Fourier series for digital signals, which is one of the fundamental concepts in \acrshort{dsp}. It takes a sequence, such as the signal represented in Figure\ref{fig:analogue-digital-signal}-b and generates $N$ complex numbers, representing power across $N$ sinusoids. The \acrshort{dtft}, as defined by classic DSP theory \cite{shenoi2005introduction}, transforms a signal $x_n$ containing samples $x_0, x_1,~\dots, x_{N-1}$ into a series $X_k$ of complex numbers $X_0, X_1,~\dots, X_{N-1}$. $X_k$ is defined by:
\begin{equation}
    X_k = \sum_{n=0}^{N-1} x_n~\cdot~e^{-\frac{i2\pi}{N}kn}
\end{equation}

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\textwidth]{DFT}
    \caption[Basic signal processing chains]{A basic chain for signal processing aimed at analysing or manipulating signal in auralisation, visualisation, or interactive applications. Analysis and processing of digital signals in the time domain is generally a hard task due to the complex nature of the function representing audio. The frequency-domain representation eases analysis and manipulation problems even with the added computational load of transforming between the two domains.}
    \label{fig:DFT}
\end{figure}

% \subsection{Image Processing Fundamentals}\label{sec:image-processing}
% \begin{itemize}
%     \item digital images
%     \item basic imge processing techniques
%     \item filters
% \end{itemize}

\subsection{Binaural Room Impulse Responses}\label{sec:ir-definition}
Common approaches to acoustic simulations involve the approximation of acoustic phenomena affecting a sound transmission occurring within a given environment between a sound source and a listener. To represent the result of such a simulation as a measurable process, where the environment is thought of as a dynamic system, Impulse Responses (IR) are used. IRs describe the effect that a system has on a sound transmission as a function of time. From DSP theory, there are several variations of IRs that the fields of immersive acoustics borrow to model several dynamic systems that affect how the human auditory system perceives soundscapes. Figure~\ref{fig:audio_rendering_chain} shows how the auditory display is affected by interconnected systems associated with aspects of the soundscape. Time invariance is the fundamental property of these systems, making it possible to model their effect as an IR by observing their response to a Dirac-Delta function, which is a function whose value is zero except at the origin, where it is infinite. In practical terms, the Diract-Delta function is an infinitely narrow energy spike often used to excite the system and obtain a response across the frequency spectrum over time. In DSP terms, the function is simply represented as a finite sequence of numbers, the \acrfull{fir}, representing amplitude levels of the output of the system over time, given an input, commonly used to measure the effects of time-invariant linear systems like amplifiers or loudspeakers.\par
In the acoustic domain, the FIR adapts to several tasks, like modelling the acoustic fingerprint of a space with respect to a source and listener by observing, at the listener, a Dirac-Delta-like signal being emitted by the source. Such IR is differentiated from standard IRs and referred to as a \acrfull{rir}; such distinction has emerged from the ongoing research in techniques and methods for measuring responses from real spaces, also due to the chaotic nature of room acoustics and real soundfields \citep{farina07}. 
IRs, as well as measuring the acoustic fingerprint of spaces, can extend as far as measuring the effect of the human auditory system on the perception of the soundscape, and there are methods for modelling how anthropometric characteristics of the human body affect sounds arriving at both ears. Such IRs are defined as Binaural Room Impulse Responses: they extend RIRs by providing individual responses for both ears. BRIRs are a representation of \acrshort{hrtf}, a function that describes how the anatomic features, rotation, and position with respect to a sound source affect the arrival of sound to the ears. Figure~\ref{fig:rir-spectrum} is an example of a monaural RIR, shown both in the time and frequency domain. 
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{rir_spectrum}
    \caption[Time and frequency-domain visualisation of a RIR]{Both time and frequency-domain, left and right respectively, representations of a Room Impulse Response (RIR). The time domain representation shows the magnitude of sound paths from a sound source to a receiver over time. The frequency domain representation shows how the energy of sound paths is distributed across the frequency spectrum over time, which is visualised with an infrared colour map.}
    \label{fig:rir-spectrum}
\end{figure}

\subsection{Measuring Real Soundfields}
A fundamental process in the field of acoustic is capturing the acoustic characteristics of physical spaces. Over the last decades, both research and industry have developed techniques for capturing soundscapes using \acrshortpl{rir}. Captured responses can encode fingerprints of unique soundfields, which are relevant to various applications like audio engineering or audio production, architectural acoustics, and virtual reality.\par
By means of the convolution operation, acoustic characteristics expressed by these responses can be applied to unpropagated audio signals. Commonly referred to as ``auralisation'', it allows rendering audio to a listener, giving the impression that it propagated within the response recording apparatus. Section~\ref{sec:real-time-conv} will discuss technical aspects and implementations of the operation.\par
The recorded impulse response can be analysed to determine acoustic parameters such as reverberation time, early decay time, and clarity \citep{farina07}. These parameters are crucial for acoustical analysis and for simulating the space's acoustics in audio production. Impulse responses allow engineers to apply the acoustic characteristics of actual spaces to studio recordings, creating authentic aural experiences in a controlled environment \citep{lima_RIR_Parameters}.\par
Over time the process of measuring soundfields with speakers and microphones has been standardised by international standards such as \citep{liebetrau2014revision}. The standard provides recommendations to maximise the representation quality of the acoustics features captured by the response. To record a response the technical apparatus may consist of a sound source that can produce a broadband audio signal, a microphone, and A/D and D/A converters to emit and capture signals from the sound system. A logarithmically swept sine is then emitted by the speaker and captured by the microphone. Ideally, the sound system should not introduce distortion into the captured signal and emitter and received should have minimal impact on the spectrum. \par
\cite{farina07} presented techniques for recovering \acrshortpl{rir} from the signal captured by the measurement microphone. The most notable technique is the convolution of the captured swept sine by the time reversal of the unpropagated sine sweep, resulting in the recovered \acrshort{rir}, Figure~\ref{fig:rir-spectrum} shown an example recovered response.\par
This process helps audio professionals and acousticians accurately reproduce and study the acoustic behaviours of different environments, enhancing audio productions and architectural designs \citep{holters2009impulse}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rir-recording1}
        \caption{RIR measurement in a large space}
        \label{fig:rir-recording1}
    \end{subfigure}

    \begin{subfigure}[t]{1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rir-recording2}
        \caption{RIR measurement in a small space}
        \label{fig:rir-recording2}
    \end{subfigure}

       \caption[RIR measurement apparatus]{Photographs showing an apparatus for recording Room Impulse Responses in real spaces, adopting the swept sine method. The method involves using a speaker to emit a logarithmically-swept sine to excite, generating a test signal that is then captured by the microphone. Through the process of deconvolution, the recorded signal can be used to produce an impulse response of the system composed by the speaker, microphone, and environment.}
       \label{fig:rir-recording-examples}
\end{figure}

\section{Common Approaches to Immersive Acoustics}\label{sec:intro-immersive-acoustics}
Achieving a convincing immersive acoustic experience is no trivial task, and various techniques and methodologies have been developed to address this complex challenge. These approaches must consider sound's spatial, temporal, and perceptual aspects and the \acrshort{has}'s intricate response to auditory stimuli. Realistic audio plays a crucial role in immersive entertainment, regardless of how a virtual environment is experienced, as it allows users to perceive the location and movement of objects and characters in a three-dimensional space, making the virtual environment more believable and engaging. 
In games or \acrshort{vr} environments where visual cues are limited or absent, high-quality spatialised audio can significantly enhance immersion and realism \citep{rubio2017immersive}. Complex sound models that include realistic sound effects such as echoes and reverberations can provide users with more information about their environment, aiding in navigation and interaction \citep{lokki2005navigation}. In virtual reality gaming, the integration of realistic audio significantly influences player experience by enhancing immersion and engagement. This effect is crucial in genres like horror or adventure, where sound contributes significantly to the atmosphere and tension of the game \citep{poeschl13}.

\subsection{3D Sound Reproduction Techniques}
Sound reproduction for immersive acoustics can be defined as a rendering problem concerning providing a listener with synthetic believable auditory stimuli perceived as belonging to a specific space. The rendering is often engineered by adopting a system that has complex scenes and scene elements as input and an acoustic signal as output. 
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{spatialiser-diagram}
    \caption[Basic spatialisation chain]{An example chain of 3D reproduction based on acoustic simulators: an environment is fed into acoustic simulators to produce BRIRs. A spatialiser system consumes these to generate an audio signal considering the listener and scene elements.}
    \label{fig:spatialiser-overview}
\end{figure}

\subsection{Spatial Audio Rendering Algorithms}
\label{sec:real-time-conv}
Audio rendering refers to the process of affecting anechoic audio with acoustic phenomena approximated by simulations. It utilises generated \acrshortpl{ir} and takes into account the characteristics of a human listener from the perspectives of a sound-perceiving object in a virtual environment and a human receiver with psychoacoustic abilities.\par
One of the fundamental operations in audio rendering is the manipulation of anechoic audio with a filter encapsulating the acoustic effect of an environment to a sound transmission, expressed as an \acrshort{ir}. Given anechoic audio expressed as a digital sequence $x$ containing $x_0, x_1,~\dots,x_n$ elements, and an IR expressed as a digital sequence $h$ containing $h_0, h_1,~\dots,h_n$, through the convolution operation $*$ we can obtain the resulting sequence $y$ containing $y_0, y_1,~\dots, y_n$ samples, expressing the resulting signal with the applied \acrshort{ir}. The following mathematical notation shows how a new function is created as a result of the convolution operation:
\begin{equation}
    y[n] = x[n] * h[n].
\label{eq:1d-convolution}
\end{equation}
In audio rendering terms, these functions will often represent an anechoic acoustic signal that is convolved with an IR to apply to create an auralised resulting signal. Given a signal $x$ as a sample sequence of $N$ points and a filter $h$ as a sample sequence of $M$ points, the resulting full convolution $y$ will be a sample sequence of $N + M - 1$ points. Each sample of the resulting $y$ sequence is the sum of the products of both sequences: 
\begin{equation}
    (x * h)[n] = \sum_{m=0}^{M}x[n-m]h[m].
\end{equation}
As shown in Figure~\ref{fig:DFT}, frequency-domain representation makes specific problems easier to solve compared to the time-domain, and convolution is one example because of the summation required in the convolution process. This summation determines the computational complexity of the operation and grows with increasing $M$ filter lengths. One key property of the convolution is that the product of the frequency-domain representation of a signal with the frequency-domain representation of a filter is the frequency-domain of their convolution. Essentially, the added complexity of summation is removed in the frequency domain at the cost of transforming the signals using the \acrshort{dtft}. Hence, audio rendering algorithms use the much faster \acrfull{fft} Convolution, commonly defined as:
\begin{equation}
    (x * h)[n] = IDTFT_N( DTFT_N(x[n]) \cdot DTFT_N(h[n]) ),
    \label{eq:fft_convolution}
\end{equation}
where $DTFT_N$ and $IDTFT_N$ are, respectively, the \acrshort{dtft} and the inverse \acrshort{dtft} of both the signal and the filter calculated over $N$ frequency points.
The Overlap-Add or the Overlap-Save are examples of real-time convolution algorithms often adopted in DSP to implement a wide range of audio effects. They are solutions to the problem of applying BRIR to long signals or to implementing interactive systems, where the listener is displayed rendered audio from a dynamic virtual environment. Thanks to the advances in such algorithms, it is now computationally feasible to manipulate anechoic audio signals with simulated acoustics on the fly, evoking a sense of immersion in the listener due to the auditory stimuli responding to changes in the dynamic environment at interactive rates.\par
The Overlap-Add algorithm adopts the divide-and-conquer approach towards an acoustic signal by segmenting an input digital sequence into multiple parts, processing the individual parts, and assembling the resulting sequence to produce a whole manipulated sequence. The goal is to evaluate Equation~\ref{eq:fft_convolution} over small chunks of audio, storing resulting convolved chunks into a queue from which samples are summed together into an output sequence.\par
Interactive audio rendering algorithms benefit from such systems as they enable on-the-fly convolution with live audio streams, which are generally implemented as a circular audio buffer. In the case of an immersive application, such audio buffers may be storing audio propagating from sound-emitting objects that interact with the user. As illustrated in Figure~\ref{fig:audio_rendering_chain}, spatialiser systems or acoustic simulation systems provide filters in the forms of IRs that can be applied to audio chunks from the audio buffer.
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{audio_rendering}
    \caption[Standard audio rendering pipeline]{Common spatial audio pipeline: the listener's position and rotation in the scene is used to sample an interpolated HRTF from the currently loaded bank, combined with the generated IR, the real-time convolution algorithm applies the BRIR to an anechoic audio signal. The result is an audio signal arriving at the listener, taking into account their position and rotation with respect to the sound-emitting object in the virtual environment.}
    \label{fig:audio_rendering_chain}
\end{figure}

\subsection{Auralisations}
The ability to auralising anechoic acoustic signals is one of the fundamental objectives in the domains of acoustics for surveying techniques, acoustics for interactive applications, and acoustics in extended reality. As seen in Section~\ref{sec:DSP-background}, there are DSP techniques that allow the application of acoustic fingerprints onto audio recordings by treating the acoustics phenomena as measurable functions that can be convolved to digital signals, see Equation \ref{eq:1d-convolution}. In higher-level terms, auralisation is the process of experiencing audio stimuli in a simulated soundscape, which can be perceiving an orchestra in a digital representation of a church, approximating how room acoustics affect the sound transmission between the orchestra and the listener in the virtual space. There are factors associated with this process that determine how well the resulting signal is able to fool the listener's auditory system into believing that the auralisation is real. Realism and presence are often a function of the performance of the components in the chain of the 3D audio reproduction system; see Figure~\ref{fig:audio_rendering_chain}.

\subsection{Common methods for Auralisations}
Methods for producing auralisation start from the creation of an environment, which is the first component of the system in Figure~\ref{fig:audio_rendering_chain} hosting the virtual sound-emitting objects, e.g. an orchestra, and the virtual sound-receiving objects, e.g. the listener. In computers, environments are generally represented using a broad range of computer graphics techniques, from simplistic Computer-Aided Design (CAD) to fully-featured virtual worlds engineered in modern game engines. Such a statement blurs the definition of a virtual environment, as one could represent a room by creating a photorealistic 3D model or by simply drawing a cuboid. Research on acoustic simulations conducted over the last decades has expanded towards defining what is required from a virtual environment to produce a believable simulation. The representation of the environment geometry is a determinant of the resolution and perceptual quality of the acoustics simulation results, and, as a general rule, the higher the level of details expressed by the geometry, the more accurate the acoustic simulator is able to simulate how sound interacts with the environment. However, beyond certain levels of details, the increase in resolution does not have a significant perceptual response \cite{pelzer2010frequency}.\par
Combined with advances in real-space scanning technology and user-friendly 3D reconstruction software, it is now possible to create appropriate virtual environments for acoustic simulations without requiring expert computer graphics engineering knowledge.\par

\subsection{Geometrical Acoustics Modelling Techniques}\label{sec:bg-raytracing}
\begin{figure}
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics{ga_is_a}
        \caption{Visualisation of the basic principle of an image source: a wavefront propagating from an emitter collides with a reflective surface. Specular reflections are computed by creating a mirrored `image source' reflected around the colliding surface.}
    \end{subfigure}

    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics{ga_is_b}
        \caption{With the definition of a receiver (blue circle), an image source can determine propagation paths from an emitter (red circle).}
    \end{subfigure}
    \caption[Image Source Visualisation]{A specular reflection calculated using the Image Source Model, demonstrated with a single reflective surface.}
\end{figure}
Geometrical acoustics is a family of acoustic modelling methods based on representing propagating sound waves with geometrical primitives, such as rays or cones. The foundations of these methods have sound propagating as straight lines, as opposed to moving particles, approximating the complex nature of acoustic energy transfer but neglecting phenomena related to the wave phenomena \citep{savioja2015overview}.\par
Ray-based techniques, wave-ray hybrid techniques, and wave-based techniques have emerged as prominent methods to generate impulse responses in the field of acoustics, each contributing to understanding how sound propagates within a given space. Ray-based techniques, rooted in geometric acoustics, simulate sound by tracing rays that emanate from a source and bounce off various surfaces within a space. This method approximates reflections and reverberation, contributing to the overall energy expressed as an impulse response.\par
Geometrical acoustics are generally efficient as they are computationally less demanding than wave-based techniques, making them suitable for real-time applications or large-scale spaces often found in cultural heritage contexts. The geometrical nature of ray-based methods allows for easier integration with virtual reconstructions of space and dynamic geometry, and the method's inherent flexibility makes it easily adjustable to different acoustic scenarios \citep{vorlander2008simulation}.

\subsubsection{Image-Source Model}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{ga_is_c}
    \caption[Image Source Model demonstration]{Visualisation of specular reflections from an emitter (red circle) to a receiver (blue circle) computed in a shoebox room. Image sources (orange circles) determine reflection points on the room geometry. The direct path (red segment) plus specular reflections computed by the Image Source Model can be used to approximate the basic acoustic features of a space.}
    \label{fig:ga_shoebox}
\end{figure}
The \acrfull{ism} is a crucial technique in geometrical acoustics, particularly effective in modelling sound reflections within enclosed spaces. This method simplifies the calculation of reverberant sound fields by treating reflections as emitted from imaginary sources.
For each real sound source and reflective surface, a corresponding image source is created on the opposite side of the surface, at an equal but mirrored distance from the point of reflection. This setup mimics the path that sound would travel if it directly reached the listener after reflecting off the surface. In environments with multiple reflective surfaces, image sources for higher-order reflections are generated recursively. Each new image source becomes a parent source for further reflections, potentially creating a complex network of sources depending on the geometry of the room and the number of reflections considered. The sound path from the real source to the listener is calculated directly. Paths from image sources are treated as if the image sources were real, with the distance and attenuation calculated based on the geometry and acoustic properties of the environment. The total sound field at the listener's position is the superposition of sound from the direct path, and all reflected paths. Each component is adjusted for delay (based on distance) and attenuation (due to both distance and material absorption properties).
%Add visualisation of source paths.

\subsubsection{Ray Tracing Techniques}
Ray tracing, expanding from \acrshort{ism} techniques, is commonly used to model how sound propagates in an environment by simulating the path of sound rays. As a core method for implementing \acrshort{ga} principles, ray tracing simplifies the sound field into discrete rays that carry sound energy, which are traced as they interact with various surfaces in a modeled space \citep{savioja2015overview}. Ray tracing can be considered a Monte Carlo method due to the approximation of acoustic reflections by means of random sampling. At a high level, the technique has rays propagating from uniformly distributed origin points across a sphere, see Figure~\ref{fig:ray-tracing-diagram}.

Ray tracing begins with the emission of rays from a sound source; they represent paths along which sound energy travels through the environment. As rays encounter surfaces, they can be absorbed, reflected, or transmitted based on the properties of the materials they encounter. Rays can typically form specular or diffuse reflections. A specular reflection occurs when rays bounce off smooth surfaces at angles equal to their incidence angles, akin to how light reflects off a mirror. Ray tracing's accuracy in modelling these reflections can vary based on the complexity of the environment, the number of rays emitted from the source and the number of bounces off geometry the model is able to compute 
\citep{thompsonRayTracing}.
Diffuse Reflections occur when sound rays strike rough surfaces and scatter in many directions; they require more complex algorithms to accurately predict the distribution of reflected sound energy.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{rt-diagram}
    \caption[Ray Tracing source visualisation]{Visualisation of a ray tracing source: In a 3D environment, a given number of rays is often emitted from uniformly distributed points across the azimuth and elevation of the emitter surface. Rays emitted from the source and colliding with geometry are reflected around the surface normal at the collision point, generating a new ray.}\label{fig:ray-tracing-diagram}
\end{figure}
Specular reflections make the assumption that rays reflect off surfaces in a single, predictable direction. This model is simpler and computationally less demanding but can miss complex interactions in environments with varied surface textures, see Figure~\ref{fig:ray-tracing-diagram2}.
Diffuse reflections scatter randomly upon striking a surface, often require probabilistic methods to compute, and can be computationally intensive but provide a more accurate depiction of realistic wave behaviors.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{rt-example}
    \caption[Ray Tracing Model Visualisation]{Visualisation of a basic Ray Tracing Model for computing specular reflections. Propagation paths from an emitter (red circle) to a receiver (blue circle) are calculated by checking whether reflecting rays intersect the receiver.}\label{fig:ray-tracing-diagram2}
\end{figure}
Some advanced ray-tracing implementations use a combination of specular and diffuse models to capture a broader range of acoustic phenomena and improve realism.
The computational load of ray tracing is generally lower than \acrshort{ism} implementations, especially at higher orders of reflection. This efficiency makes ray tracing advantageous for scalability as it can more easily scale to larger and more complex environments \citep{schissler2014high}.
Fast implementations also enable real-time applications and make it feasible for integration into game engines.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{rt-full-example}
    \caption[Ray Tracing, shoebox room demonstration]{A Ray Tracing model computing specular reflections in a shoebox room. The example shows that the model lacks the ability to simulate diffraction effects caused by sound bending through a portal \citep{funkhouser:inria-00606737}.}\label{fig:ray-tracing-diagram3}
\end{figure}
Fast ray tracing implementations also allow for higher reflection order, achieving realism in simulating acoustic phenomena, whereas the \acrshort{ism}'s computational cost increases exponentially with each added reflection order.
While ray tracing is versatile and efficient, it still carries \acrshort{ga} limitations, such as the high-frequency bias. Ray tracing tends to be more accurate at higher frequencies, where the wavelength is much smaller than the objects and spaces involved, and a ray can better approximate the path of a propagating wave. At lower frequencies, its accuracy diminishes as wave effects like diffraction become more significant as rays are unable to bend around obstacles, see Figure~\ref{fig:ray-tracing-diagram3}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{source-paths}
    \caption[Geometrical Acoustic source paths visualisation]{Diagram showing the anatomy of a reflection. Geometrical Acoustic techniques consider source directivity profiles and receiver descriptions (human listeners). The diagram shows how a reflection is generated from the source and is affected by source characteristics (directivity profiles). Bounces off the geometry and environment characteristics influence the reflection path through materials. Finally, Head-Related Transfer Functions can be used to produce binaural Impulse Responses \citep{schroder2011physically}.}\label{fig:ga-source-paths}
\end{figure}


\subsubsection{Wave-based Modelling Techniques}
\begin{figure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fdtd1}
        \caption{FDTD simulation timestep 1}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fdtd2}
        \caption{FDTD simulation timestep 2}
    \end{subfigure}

    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fdtd3}
        \caption{FDTD simulation timestep 3}
    \end{subfigure}
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fdtd4}
        \caption{FDTD simulation timestep 4}
    \end{subfigure}
    \caption[Visualisations of Finite-Difference Time-Domain simulation timesteps]{Visualisations of Finite-Difference Time-Domain simulation timesteps. Finite-Difference Time-Domain (FDTD) is a computational technique used to model sound propagation in virtual environments by solving the wave equation at discrete space and time intervals. The figures represent several timesteps of a running simulation, showing how sound pressure emitted from a source changes across timesteps.}\label{fig:fdtd-timesteps-example}
\end{figure}
Wave-based techniques stand out for their precision, solving the wave equation to simulate how sound waves propagate through space, accurately modelling diffraction, scattering, and other complex wave phenomena. While highly accurate, wave-based techniques often require substantial computational resources, making them less suited for real-time or large-scale applications \citep{raghuvanshi2014parametric}.\par
Wave-based methods account for the full complexity of sound waves, including diffraction, interference, and wavefront curvature—phenomena typically ignored by geometrical acoustics. These methods are based on solving the wave equation, which describes how sound pressure levels vary in space and time.
Amongst popular techniques for sound propagation are \acrfull{fdtd} methods. These methods discretise the wave equation in both time and spatial domains, using a grid to simulate how waves propagate through a medium \citep{hamilton2017fdtd}. \acrshort{fdtd} methods are widely used in engineering and physics to model sound propagation in complex environments and to study the effects of diffraction and absorption or to model natural phenomena outside the sound domain \citep{teixeira2023finite}.
In \acrshort{fdtd}, the simulation domain is the space truncated by the simulation region and discretised by the mesh. When an FDTD simulation runs, the acoustic energy fields are calculated from wave equations in every mesh cell and the solutions are repeatedly time-stepped, see Figure~\ref{fig:fdtd-timesteps-example}. Spatial discretisation allows for the representation of complex geometries and structures, while temporal discretisation captures the evolution of energy fields over time.
These techniques can model wave phenomena with high accuracy and can account for varying medium properties but at the cost of high computational resource requirements. In large environments, they can express complex behaviours associated with wave propagations, like sound bending around obstacles or portals, enabling the generation of realistic stimuli and a coherent soundscape, see Figure~\ref{fig:wavefield-coding}. However, the computational costs associated with running simulations and computing the wave equation can limit their application for certain tasks.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{wavefield-coding}
    \caption[Demonstration of the parametric wavefield coding method]{Demonstration of a state-of-the-art wave-based acoustic renderer: pressure fields are pre-computed within a complex virtual environment and encoded to allow game engines to decode and auralise sound sources in real-time \citep{raghuvanshi2014parametric}.}\label{fig:wavefield-coding}
\end{figure}
Alongside \acrshort{fdtd} techniques, Boundary Element Methods solve the Helmholtz equation, a form of the wave equation applicable to steady-state problems, for the boundaries of a domain, reducing the dimensionality of the problem from the volume to the surface. They subdivide a large problem (such as a room or an outdoor environment) into smaller, simpler parts called finite elements. The sound field in each element is approximated by basis functions. Commonly used in architectural acoustics and the automotive industry, the technique helps in designing quieter and more acoustically pleasant spaces, designing spaces with specific acoustic properties, such as concert halls and lecture theatres, assessing the impact of noise on communities and designing sound barriers to mitigate unwanted noise \citep{gumerov2021fast}.
Boundary methods are particularly effective for exterior problems, such as noise propagation in an open environment, or noise profiling.
They reduces the problem size significantly which can decrease computational demands.
The method is less effective for high-frequency sounds where the wavelength is small relative to the dimensions of the modelling domain, as the surface elements need to be sufficiently small. They are highly accurate and versatile, capable of modelling complex material properties and geometries but computationally intensive, especially for three-dimensional problems and high frequencies \citep{kirkup2019boundary}.


\subsubsection{Hybrid Modelling Techniques}
On the other hand, wave-ray hybrid techniques present a more complex picture, combining aspects of ray-based and wave-based methods. Rays are utilised to model the high-frequency components of the sound, while wave equations handle the low-frequency behaviour, attempting to capture the best attributes of both methods. However, the hybrid nature often means more computational resources are needed, and it might not always be the most suitable choice for fast applications where platforms offer limited computational resources \citep{hulusic2012acoustic}.
An example experimental method from~\cite{southern2013room} shows how a physical FDTD model is used for low-mid frequencies, while high frequencies are handled using beam-tracing or acoustic radiance transfer methods. This approach ensures a balance between computational load and physical accuracy.


\subsection{Summary}
Ray-based techniques offer a compelling option for interactive applications. Their computational efficiency, relative simplicity, and adaptability in handling various scenarios effectively capture the essential acoustic characteristics of sounscapes. Unlike wave-based or hybrid methods, ray-based techniques can easily adapt to dynamic environments, aligning with the objectives of this work and constraints often found in \acrshort{ar} platforms. Therefore, while the high accuracy of wave-based methods or the comprehensive nature of hybrid methods may have specific applications in areas of high-fidelity and complex propagation effects, it is the ray-based techniques (such as those employed by acoustic simulation software such as ODEON) that generally stand out as the most appropriate choice for fast and efficient acoustic simulation in immersive applications.

% ---------------------------------------------------------------------------------
% * Virtual Environments
% ---------------------------------------------------------------------------------
\section{Virtual Environments}
Room acoustics simulations may involve the concept of virtual environments to represent sound-emitting objects, receivers, and space where these exist. Computer games technology has shaped the definition of virtual environments over decades of development and progress.

\subsection{Representation of Virtual Environments}
Graphics rendering pipelines display objects of a complex scene to viewers, determining the appearance of materials and geometry of the environment. In VEs, meshes are composed of triangles enabling game engines to organise geometry based on the semantics of scene objects. For E.g. a mug can be represented by triangles grouped in a mesh. They are essentially a network of triangles that connect, having adjacent vertices, to form objects. They are responsible for transforming the scene geometry and applying further processing, such as rasterisation, which generates fragments from geometry combined to create frames. A series of frames generated at interactive rates compose a frame buffer that allows users to experience scenes in real-time. Graphics pipelines describe geometry as vertices and triangles, applying shading techniques to control the appearance of surfaces depending on their lighting conditions and the viewer’s spatial position. Here, texture images can also define the appearance of objects' geometry by painting their surfaces and controlling transparency \citep{mcallister2002efficient, marschner2015fundamentals}.\par
Textures can determine the appearance of material composing objects in a scene adopting two-dimensional images. Texture mapping uses colour and transparency information contained in these images to paint triangles forming the geometry. Texture coordinates provide graphics pipelines with enough information to paint meshes.

\subsection{Handling of Complex Scene Geometry}\label{sec:bg-geometry-handling}
Implementing multimodal interactions in \ACRshortpl{ve} often requires handling and performing operations on the scene geometry, including searching interactions between entities and the environment. In computer games, physics systems are often fundamental components enabling game mechanics and interactions, which often involve computing intersections between scene entities and the environment. With the growing density of environment geometry and complexity of the scene elements, the computational requirements associated with evaluating these geometry searches have grown, demanding optimal solutions across the space and time domains.\par
The goal of geometry handling systems is to allow searching intersections between volumes or primitives, such as rays or frustums, and the scene geometry and the engineering design of such systems are closely related to data structures and algorithm design. Data structures space and time complexity
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
       \centering
       \includegraphics[width=\textwidth]{bvh1}
       \caption{Triangulated Mesh}\label{fig:sub_bvh1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
       \centering
       \includegraphics[width=\textwidth]{bvh2}
       \caption{Bounding Volumes}\label{fig:sub_bvh2}
    \end{subfigure}

    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{bvh3}
        \caption{Tree}\label{fig:sub_bvh3}
    \end{subfigure}
    \caption[Visualisation of a Bounding Volume Hierarchy]{A bounding volume hierarchy constructed on a given input scene represented as a triangulated mesh (a). Bounding volumes encapsulate mesh primitives, (b), which then represent nodes of the tree, (c).}\label{fig:bvh-diagram}
\end{figure}


\subsubsection{Binary Space Partitioning}
One of the first approaches in handling and indexing scene geometry in \acrshortpl{ve} is \acrfull{bsp}, which, motivated by performance aspects and limited computational resources available during the early developments of rendering pipelines in the field of computer graphics \citep{fuchs_bsp}. \acrshortpl{bsp} allow graphics pipelines to organise the order of scene elements before drawing them or to determine the visibility of surfaces.\par
The goal of \acrshortpl{bsp} is to index and search scene elements or geometry primitives, part of a given input scene. The technique works by subdividing the Euclidean space in which scene elements exist. The space is divided by partitioning planes, separating scene elements based on which side of the plane they exist. The process repeats recursively, subdividing space with further partitioning planes. Several criteria can determine the number of further subdivisions, such as the minimum size of regions generated by space subdivisions or the indexing complexity and granularity required for indexing and searching operations.\par
With subdivided regions obtained with partitioning planes, a binary tree, similar to the diagram shown in Figure~\ref{fig:bvh-diagram}, where a root node refers to the entire scene and branches into the first space subdivision, which recursively branches into further subdivisions, until a ``leaf'' region. A leaf region can hold a scene element, a geometry primitive, or a subset of primitives from the set of primitives representing the input scene.\par

\subsubsection{Bounding Volume Hierarchies}
A \acrfull{bvh} is a method closely related to \acrshort{bsp} for handling scene geometry that optimises intersections between rays and the scene by adopting a binary tree to subdivide primitives that compose the scene geometry. A \acrshort{bvh} can represent a scene by constructing a binary tree partitioning geometry primitives into a hierarchy of disjoint sets. In physically-based rendering applications, mesh triangles are often the primitives indexed by the constructed tree; see Figure~\ref{fig:bvh-diagram} \citep{pharr2023physically}.\par
In a tree, bounding volumes are generated to fit primitives from a given triangulated mesh (Figure~\ref{fig:sub_bvh1}) and aggregated based on proximity (Figure~\ref{fig:sub_bvh2}). Bounding volumes encapsulating multiple primitives generate branches, and recursively, branches are encapsulated in volume until a root volume fits the entire input scene. A constructed tree (Figure~\ref{fig:sub_bvh3}) can be queried and traversed by navigating branches from the root node to primitives within leaf nodes.\par
Thanks to branch subdivisions, ray-volume intersection tests performed on nodes allow filtering out entire segments of the scene, reducing the set of primitives that potentially intersect the ray to a subset of the input scene triangle set and improving the space complexity of the operation, much like in \acrshort{bsp} techniques. Recent research trends are exploring tree rotations and balancing of branches in real-time, optimising search operations even further, and allowing the tree to reflect dynamic changes to the scene geometry \citep{kopta2012fast}.
 
% \subsection{Sound Sources in Virtual Environments}
% Representing space in virtual environments, capturing real space and synthetic scenes.
% - Game Engine architecture book                             

\subsection{Materials}\label{sec:bg-materials}
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{brdf1}
    \caption[Basic material system in virtual environments]{A simplistic material system visualised as a ray of light emitted by a source and colliding with a surface. The ray reflects around a surface normal and is detected by a virtual camera.}\label{fig:material-vis}
\end{figure}
In computer graphics and multi-modal rendering, assigning physical properties to surfaces of the scene geometry has been addressed with the definition of materials. The definition of materials is often intrinsic to the rendering technique and to the engineering design of the rendering apparatus. In physically-based graphics rendering techniques,~\cite{pharr2023physically} define materials as a
\begin{quotation}
    ``description of its appearance properties at each point on the surface''.
\end{quotation}
\subsubsection{Materials in Rendering Pipelines}
Rendering pipelines often model how surfaces in complex scenes reflect and respond to propagating energy by employing \acrfullpl{brdf}. In the light domain, rendering techniques often model reflected energy as a function $f_r(p, \omega_i, \omega_o)$ of a $p$ \acrshort{brdf}, an incoming direction $\omega_i$ and an outgoing direction $\omega_o$. A simplified diagram in Figure~\ref{fig:material-vis} shows how energy transmits from a light source and is sampled by a virtual camera, with a surface reflecting the light ray around the surface normal at the collision point. Though, materials in the real world have unique physical properties affecting reflectance, absorptions, or diffusion of incidental light, deviating from the idealised model shown in Figure~\ref{fig:material-vis}.\par
A material using a reflectance function can model realistic behaviour, allowing surfaces to express varying physical attributes like roughness or metallic characteristics. Figure~\ref{fig:brdf-vis} shows example functions simulating a rough and a glossy material, Figure~\ref{fig:brdf1}~and~\ref{fig:brdf2} respectively. These examples show \acrshortpl{brdf} modelling the scattering of energy caused by the rough surface and the glossy reflections caused by a mirror-like surface.\par

\subsubsection{Materials in Sound Rendering}
Defining materials translates to the acoustics domain, applying closely related principles defined in the visual domain. \ACRshort{ga} methods often share the same approach shown in Figure~\ref{fig:material-vis} by considering sound in \acrshortpl{ve} as propagating rays (or other geometry primitives) colliding with surfaces that reflect energy based on attributes assigned to the surface.\par
In real soundfields, acousticians and architects often plan the presence of certain materials to control aspects of sound propagation within a given environment. Studies show that strategic placements of surfaces with high acoustic absorption characteristics can have a positive subjective influence on perception in environments, improving the clarity of acoustic information transmitted within the space \citep{arvidsson2021subjective}. Absorption panels, diffusers, or bass traps are some example materials and surfaces that acousticians use to control how acoustic energy reflects around the environment, controlling parameters like $T_{30}$ or $T_{60}$ reverberation metrics or $C_{50}$ and $D_{50}$ clarity and definition metrics, respectively.\par
Modern game engines and acoustic simulation software aim to replicate the behaviour of these surfaces by encoding acoustic characteristics to scene geometry representing an environment. In \acrshort{ga} simulation methods, material characteristics like absorption or scattering coefficients can influence of geometry primitive simulating propagating sound and interact with the environment, similarly to \acrshortpl{brdf} \citep{rindel2000use}. Finally, acoustic material can encode frequency-dependent acoustic information, often expressed around \acrfull{erb} frequency region, to consider aspects of the \acrshort{has}. Chapters~\ref{ch:Materials}~and~\ref{ch:acousticrendering} will discuss the use of materials in the context of the overarching aim of this thesis.

\begin{figure}
  \centering
    \begin{subfigure}[t]{0.49\textwidth}
       \centering
       \includegraphics[width=\textwidth]{brdf2}
       \caption{Rough Material}\label{fig:brdf1}
    \end{subfigure}
  \hfill
    \begin{subfigure}[t]{0.49\textwidth}
       \centering
       \includegraphics[width=\textwidth]{brdf3}
       \caption{Glossy Material}\label{fig:brdf2}
    \end{subfigure}
  \caption[Visualisation of example BRDFs]{Example \acrshortpl{brdf} applied to a surface, defining a rough material (a) and glossy material (b). These functions emulate how properties of real surfaces respond to colliding light: glossy materials will reflect energy specularly, whereas uneven rough surfaces will cause diffuse scattering.}\label{fig:brdf-vis}
\end{figure}

\section{Deep Learning Background}
Subsequent chapters of this thesis will leverage deep learning techniques to solve a subset of problems associated with developing a system targeting the overarching aim. Specifically, acoustic materials are central to applying the system to realistic, complex scenes, as they contribute towards the perceived quality and realism evoked by the auditory display. The problem arises from the complexity of mapping the appearance of surfaces within the complex scene to acoustic materials. Many factors in complex virtual environments influence the appearance of surfaces, making it hard to distinguish surfaces and map them against acoustic materials automatically.\par
Deep learning is a subset of machine learning comprising techniques and pipelines to address such mapping problems by learning from examples and providing a generalised model for unseen cases. The potential of deep learning lies in the feature extraction process, allowing models to learn from examples influenced by many factors. The term deep learning is associated with the feature extraction process, delegated to layered feature extraction components composing the model \citep{dolhasz2021perceptually}.\par


\subsection{Image Processing}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{image-processing-example}
    \caption[Image processing example --- Gaussian filter]{Visualisation of an image processing technique applied to an example image (left axes). On both axis pairs, abscissa and ordinate indicate pixel coordinates. A Gaussian blur filter is applied to the image by means of convolution applied to pixel intensity values representing the image. The right axes show the result.}\label{fig:image-gaussian-filter}
\end{figure}
Image processing is the act of performing operations on an image in order to enhance it or extract information. It involves manipulating pixel data to improve the image or to analyze it. This field intersects computer science, mathematics, and a broad range of engineering disciplines. Similarly to digital audio signals, as overviewed in Section~\ref{sec:DSP-background}, images are often represented in digital systems by encoding intensity values of colours registered by an analogue-to-digital converter \citep{marschner2015fundamentals}.
Figure~\ref{fig:image-gaussian-filter} shows an example image processing operation where pixel values expressing pixel intensity values are represented as $\mathcal{RGB}$ triplets and expressed as a three-dimensional matrix that spans across the width and height of the image in pixels. Through convolution, a Gaussian filter is applied to the pixel values, rendering a processed image.\par
Image processing is fundamental to deep learning as operations like convolution are often used to apply filters for several applications \citep{goodfellow2016deep}. Convolutional neural networks build upon the process of decomposing a given image with multiple filters, from coarse to fine, matching patterns and shapes, expressed through kernels, which can be learned by neural networks as the next Sections will demonstrate.\par

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{nn}
    \caption[Diagram of a fully connected neural network]{A visualisation of a basic neural network with fully connected hidden layers.}\label{fig:nn}
\end{figure}

\subsection{General Machine Learning Tasks and Applications}
Deep learning techniques discussed in this thesis can be broadly categorised based on the learning approach into supervised learning and unsupervised learning. Supervised learning is an example where the algorithm learns from a labelled dataset, understanding the relationship between the input features and the target output. The goal is to predict the output for new, unseen data based on this learned relationship. Unsupervised learning algorithms, on the other hand, analyse unlabeled data to find patterns or inherent structures.\par
The goal of a model is to provide inference on unseen data based on training on a set of representative examples, emulating basic human abilities that are hard to programmatically engineer in computers. Typically, deep learning models consist of an input layer ingesting data such as images or audio signals, deeper layers extracting features from input data, and output layers transforming the extracted features to perform a task. A loss function is often employed to measure the error and accuracy of the output data and improve the fitness of the model through backpropagation: the tuning of weights and biases of neurons in hidden layers based on computer errors, see Figure~\ref{fig:nn}. With layers fit on a dataset, the model can infer output from unseen data, generalising on the task at hand \citep{szeliski2022computer}.
General deep learning tasks related to this thesis include classification, regression, or synthesis.
Classification is a type of supervised learning where the goal is to predict the category or class of an input. The input data is fed into the algorithm, which then outputs a label from a predefined set. For example, a classification model might be used to predict the presence of objects in an input image. Classification can be binary (two classes) or multiclass (more than two) \citep{goodfellow2016deep}.\par
Regression, another type of supervised learning, involves predicting a continuous quantity instead of a categorical label. The aim is to find the relationship or mapping between input variables and a continuous output variable. An example application is to predict parameters of audio engines like pitch or amplitude based on input physics factors like mass \citep{colombo2021psychometric}. Regression models are evaluated using different metrics than classification models, such as \acrfull{mse}. The choice of metric often depends on the specific requirements of the task.\par
Deep learning techniques are becoming increasingly popular in virtual environment pipelines due to the flexibility and potential to adapt to various tasks.

\subsection{Deep learning Tasks Within Immersive Applications}
Detecting the presence of certain objects in an image represents a milestone in the development of \acrshortpl{cnn} and computer vision techniques as it emulates a basic task of the human visual system. Due to the nature of image representations in computers, as described in Section~\ref{sec:image-processing}, recognising entities depicted by images is a central problem in computer vision \citep{szeliski2022computer}. Classic computer vision algorithms have approached the problem by providing algorithms to recognise patterns programmatically by filtering the image or scanning for certain features. Thanks to advances in \acrshortpl{cnn}, object detection was addressed by extracting features using deep layers and learning from annotated examples expressing a set of classes captured in various contexts.\par
\paragraph{Object Detection}
Pioneering large-scale labelled datasets, such as the work by~\cite{deng2009imagenet} on ImageNet, enabled object detection networks to improve their efficiency and abilities of recognising classes. Of pioneering importance is~\cite{Redmon_2016_CVPR}'s You Only Look Once (YOLO) network that introduced a state-of-the-art solution able to recognise thousands of classes with high accuracy and precision.\par
\paragraph{Image Segmentation}
Similarly to object detection, the task of image segmentation involves dividing a three-dimensional scene into its constituent entities, with the aim of identifying and categorising different segments based on features extracted. This task is crucial in robotics, autonomous driving, and \acrshort{ar}, where understanding the structure and layout of the environment is essential for navigation and interaction. Techniques like point cloud segmentation and voxel-based approaches are commonly employed, leveraging deep learning models to process and classify 3D data \citep{minae_segmentation, feng2020semantic3d, kalogerakis20173d}.\par
\paragraph{Pose Estimation}
Pose estimation refers to the task of determining the position and orientation of objects or individuals within a scene. In human pose estimation, this typically involves recognition of gestures from \acrshort{hmd} cameras to enable human-computer interaction \citep{andriluka20142d, spittle2022review}.\par
\paragraph{Scene Reconstruction}
Scene reconstruction tasks can create a complete 3D model of a scene from a series of images or video frames or generally sparse input information. This can involve reconstructing the geometry of the environment, textures, and lighting conditions \citep{patow2003survey}.\par
\paragraph{Sound Source Separation}
Sound source separation tasks involve isolating individual audio sources from a mixture of sounds. This is common in audio engineering to improve the clarity of speech in noisy environments, for instance. Techniques often involve signal processing methods and machine learning models designed to distinguish between different sound characteristics \citep{virtanen2006sound}.\par
\paragraph{Audio Scene Understanding}
Audio scene understanding is the process of interpreting audio signals, detecting the presence of auditory elements like footsteps or speech, and discerning the context or setting. It is akin to scene recognition in computer vision but applied to auditory inputs \citep{abesser2020review}.\par
\paragraph{Sound Propagation Modelling}
Sound propagation modelling involves using deep learning models to simulate how sound waves travel and interact with the environment, including reflection, absorption, and diffraction around obstacles. This is crucial in acoustics engineering, game development, and architectural design to create realistic sound environments and to analyse the impact of sound in physical spaces \citep{liu2022sound}.\par
\paragraph{Measuring Perceptual Similarity}
Perceptual similarity tasks involve determining the perceptual distance between two stimuli and emulating human perception rather than computing pixel-level or waveform similarities. This requires understanding the features that humans consider important in judging similarity, a task with applications in image retrieval, content recommendation, and quality assessment \citep{Dolhasz_2020_CVPR}.\par
\paragraph{Autonomous Behaviour Modelling}
Agency in games and reinforcement learning are deep learning approaches to simulate the capacity of players (or entities within a \acrshort{ve}) to make choices and perform actions that affect the environment. Example uses of reinforcement learning include training agents to perform navigation in complex 3D environments using audiovisual stimuli, or training a robot to perform actions in a virtual environment that can be mirrored in the physical world \citep{yannakakis2018artificial, matulis2021robot}.

% \subsection{Audio Scene Understanding and Source Separation}
% Reasoning and performing tasks on auditory information are analogous to computer vision problems due to the same nature of digital representations.

% Sound source separation, also known as audio source separation or audio source separation, is a process in audio signal processing that aims to separate individual sound sources from a mixture of sounds. It is the task of isolating and extracting specific audio sources from a recording where multiple sound sources are present. In many real-world scenarios, such as music recordings, conversations, or environmental recordings, multiple sound sources contribute to the overall audio signal. Sound source separation techniques are employed to enhance the clarity and quality of individual sources, making it easier to analyze, process, or manipulate specific elements within the audio.

\section{Conclusions}
The current state of interactive sound rendering allows for fast acoustic simulations, even on platforms with limited computational budgets, approximating the soundfield of any given environment, where a listener can experience realistic auditory interactions with virtual sound sources \citep{lakka2018spatial, hulusic2012acoustic}. Sound rendering can be considered a fundamental component of computer games technology, responsible for reproducing everyday sound emitted by objects or agents in a virtual scene and perceived by a listener. This poses the challenging task of reflecting basic acoustic principles to render such auditory interactions realistic. In the real world, sound propagates from a sound source to a listener and interacts with objects in the environment and with the environment itself arriving at the listener's ears \citep{kuttruff2016room}. Sound cues alone are sufficient to enable users in Virtual Environments (VEs) to pinpoint locations of sound-emitting entities in a scene by using auditory sound localisation, a natural ability associated with the human auditory system \citep{lokki2005navigation, rubio2017immersive}.\par
As the acoustic principles that govern how sound propagates in space are difficult to reproduce in digital systems, many methods exist, providing variable orders of approximations, depending on the application. Such approaches emulate the wavefield of an environment, simulating how sound interacts with boundaries and scene objects. A subset of these can reproduce phenomena of sound, such as diffraction, reflection, and refraction, which are determinants of realism as they emulate how waves bend around obstacles. Such phenomena make the simulated wavefield dependent on the accuracy of scene geometry and materials represented in a VE. \par
There is a large tree of techniques and methods to simulate sound propagation, reflecting acoustic properties to any given sound source in a VE, adapting to perceptual requirements and computational budgets available \cite{doukakis2019audio}. As a general rule, the more computational budget available, the more complex techniques can be employed, allowing realistic sound rendering. Finite-difference Time Domain (FDTD) approaches shown by \cite{hamilton2017fdtd}, or wave-based by~\cite{raghuvanshi2014parametric} methods, on this end of the spectrum, obtain high degrees of accuracy and realism, but often require pre-computation stages or GPU implementations to produce acoustic simulations at interactive rates. Wave-based methods provide a detailed and rigorous approach to acoustic modelling, ideal for scenarios where high fidelity and accuracy are necessary, particularly when dealing with low frequencies and complex interactions. While computationally more intensive than geometrical acoustics methods, the depth and realism they offer make them indispensable in many advanced acoustic studies and applications.\par
On the other end of the spectrum, there are fast geometrical acoustics methods, widely adopted in real-time applications due to their low computational requirements and highly parallelisable implementations \citep{cowan2010gpu}, which reduce simulated sound waves to rays or beams, that are much simpler to compute. Ray tracing is a robust and versatile technique in geometrical acoustics, widely used for modelling room acoustics due to its computational efficiency and adaptability to different reflection models. While it offers broad applicability and scalability, its accuracy in certain scenarios, particularly in complex, diffuse environments and at lower frequencies, may be limited. Finally, hybrid methods also exist to combine the strengths of the main families.\par
Acousticians and engineers have always employed classic sound rendering to solve practical problems as it requires the work of experts to adjust parameters and define the acoustic characteristics of a virtual scene. A constant here is the requirement of an accurate description of the environment, detailing the geometry of architectural components and objects contained within with acoustic information such as acoustic energy absorption, reflection, or scattering --- this is essential to model the behaviour of sound waves interacting with the environment. \par
Only recently, with the increase of processing power available in computers, it has gained popularity in computer games and immersive technology for entertainment and serious applications \citep{zhang18}. \acrshort{ar} technology can particularly benefit from this as the increase in processing allows sound rendering on mobile devices, enabling listeners to experience virtual sound sources propagating in the reconstruction of real geometry, which is the main avenue that the planned thesis work aims to explore.\par

Hearing is a complex function that requires modelling of a human listener as well as considering environmental aspects.

% Acoustic materials
% Deep learning for acoustic materials
% Next up: lit review
