\chapter{Conclusion}\label{ch:conclusion}
The primary goal of this thesis has been to explore a novel system for acoustic rendering bespoke to \acrshort{ar} platforms, enabling context-aware rendering to consider characteristics of the environment surrounding the player. The objectives defined by this thesis developed and evaluated individual systems composing the pipeline proposed as a solution modularised into standalone contributions. Overall, the development of components evaluations conducted on test cases showed that the domain of acoustic rendering for \acrshort{ar} would benefit from applications of acoustic rendering pipelines, and the discussions on the results gathered open several potential avenues for future research heading towards more interactive and realistic auditory interactions.

\section{Summary of Contributions}
The following Sections will reflect on the objectives posed by the introductory chapter of this work (Section~\ref{sec:thesis-objectives}), highlighting significant results by connecting findings gathered from studies conducted throughout the development of the pipeline. For contextualisation, clarity, and accessibility, Table~\ref{tab:objectives-contributions} summarises the thesis aims and how Chapters contribute to each objective.

\input{objectives-contributions}

Overall, the significance and contribution to knowledge of this thesis work lies in the proposal of a novel rendering pipeline, expressed by Chapter~\ref{ch:ar-pipeline}, which illustrates a design of the end-to-end system, targeting the primary objective, illustrating its concept and design principles, demonstrating implementations and apparatus design, and outlining the vision and potential expansions.\par
Chapter~\ref{ch:evaluation} evaluated a prototype deployed to an \acrshort{ar} platform, evaluating psychoacoustic factors in auditory displays, demonstrating the validity of the overarching aim by proving that a pipeline for realistic auditory displays has effects on interactions in virtual environments.\par

\subsection{Integration of Computer Vision into Sound Rendering}
As a high-level discussion around the interpreted results gathered from evaluating the system and its components, there are conclusions drawn that determine contributions towards the field of \acrfull{aar} and domains of computer graphics and vision in the context of simulating soundfields and generating realistic auditory stimuli. The following Sections will elaborate on the significance of the contributions, discussing their relevance to the broader research domains, target use cases, their limitations, and how future work can overcome them.

\subsection{System Overview}
A prototype system for realistic auditory displays in \acrshort{ar}, considering basic features of the user's surrounding environment, has shown potential in evoking perceptual responses within applications and tasks in immersive experiences. Extrapolating from research trends in acoustic rendering domains, as shown in Chapter~\ref{ch:lit-review}, deep learning-based real-time sound rendering is becoming increasingly accessible and feasible to deploy in \acrshort{xr} platforms. A summary of related work concerning areas of sound rendering and computer vision was discussed in light of the overarching aim of the thesis, targeting Objectives~\ref{row:o1} to~\ref{row:o6} in Table~\ref{tab:objectives-contributions}.
The modularity of the system proposed to address the thesis objective provides methodologies and evaluations that can apply to future extensions, facilitating the use of experimental sound rendering for wearable computing.

\subsection{Acoustic Characteristics Retrieval Methods}
The solutions proposed to target Objectives~\ref{row:o2} and~\ref{row:o3} addressed the problem of extracting soundscape features in context-aware acoustic rendering target platforms equipped with space-sensing technology. The two proposed novel systems addressed the problem of retrieving acoustic characteristics from environments from their visual representations by adopting computer vision techniques. This thesis work and recent methods show that such a task is still an open research question due to the complexity in the mappings between visual representations of space and auditory stimuli generated from simulated soundfields based on properties assigned from inferring their visual appearance.\par
Despite deep learning methods exist to create mappings between semantics and visual characteristics of materials and surfaces \citep{gaur2019superpixel,chen2022visual}, there are still open challenges around the areas of recognising and attributing properties to virtual geometry via its visual features.
Mapping acoustic properties to representations of space has been defined in this work as the ``material tagging'' process, expressing the task engineers face when constructing immersive, complex scenes in game engines, where portions of scene geometry are assigned acoustic material properties. As acoustic simulations become more feasible for increasing use cases and applications \citep{liu2020sound}, material tagging will incur higher manual authoring, which the application of computer vision techniques can automate, enabling and rendering procedural workflows \citep{schissler2017acoustic}.\par
There are benefits to the two proposed systems that aim at automating material tagging. With the camera-based acoustic material tagging system demonstrated in Section~\ref{sec:camera-tagging}, a novel system that uses camera renders to infer portions of scene geometry they depict, provides a solution that can adapt to cameras equipped by \acrshortpl{hmd}. It allows the system to scan and map acoustic characteristics to the environments as the user walks around their \acrshort{ar} scene.\par
The texture-based alternative system demonstrated in Section~\ref{sec:texture-tagging} expanded towards a more efficient workflow that also has applications to \acrshortpl{hmd} by using spatial reconstruction systems that provide and update scene geometry as the user navigates the scene.

\subsection{Acoustic Rendering}
Geometrical acoustics-based rendering pipelines have the advantage of controlling the number of rays and other aspects associated with the simulated acoustic phenomena. Thus, in order to target Objectives~\ref{row:o4} and~\ref{row:o5}, perceptual evaluations aimed at measuring psychoacoustic factors associated with auditory stimuli produced from simulated soundfields benefit from control over the resolution of the sound propagation technique. The degree of control is crucial for evaluating perceptual responses and defining the minimum requirements for psychoacoustic factors of the generated stimuli.\par
Chapter~\ref{ch:acoustic-rendering} provided important insights into the development of sound rendering pipelines targeted at wearable computing platforms, adopting geometrical acoustics techniques. The testing performed on renderer prototypes generated data on the deployment of the renderers on virtual and real real-world scenes, providing crucial discussion points around realism and performance aspects of seleced rendering techniques.\par
The current state of the field of sound rendering has emerging techniques that range from classic approaches to sound propagation such as geometrical acoustics \citep{savioja2015overview} or deep learning methods like \cite{Singh_2021_ICCV}'s Image2Reverb. However, there is a lack of literature around developing and testing techniques for \acrshortpl{hmd} or the wider field of wearable computing platforms. The work towards Objective~\ref{row:o5} is a step towards outlining guidelines for the development of sound rendering pipelines for wearable computing platforms, considering limited resolution sound propagation and geometry reduction paradigms.

\subsection{Objective and Subjective Evaluations}
Despite the modularity of the proposed system and the standalone nature of individual components developed, evaluations and preliminary tests to evaluate their performance, efficacy, and human factors require consideration of the entire pipeline, including the final link of the chain, the listener. This final link is essential for determining perceptual responses and conducting ablation studies to evaluate the effect and the priority of individual components, targeting Objectives~\ref{row:o5} and~\ref{row:o6}.\par
Of significant impact here are the methodologies in Chapters~\ref{ch:materials} and~\ref{ch:acoustic-rendering}, where cutting-edge perceptual encoding techniques are leveraged to optimise preliminary testing of perceptual factors of systems for retrieving acoustic features from virtual environments. For instance, employing a deep perceptual audio similarity metric allows fast preliminary testing of the sound rendering pipeline, eliminating the need to recruit human subjects.

\section{Limitations}
Despite the contributions achieved through this thesis work in the integration of sound rendering with computer vision, it is important to acknowledge the inherent limitations that need to be considered when interpreting findings. This section elaborates on specific limitations relative to the proposed system, and the experiments conducted range from the scalability of our algorithms in real-world applications to the generalisability of our results across different environments and scenarios. By openly discussing these aspects, we aim to provide a comprehensive understanding of the context within which our findings should be interpreted, thereby enriching the discourse and encouraging subsequent innovation in the field.

\subsection{Acoustic Material Tagging}
A limitation of the acoustic material tagging system arises from the methodology used for acoustic validation. Specifically, the testing protocols did not include comparisons against measurements obtained from impedance tubes, which are standard in evaluating the acoustic properties of materials. Impedance tubes offer precise data on how materials absorb and reflect sound, providing a critical benchmark for the accuracy of any sound rendering system. The absence of this comparison means our system's effectiveness in accurately rendering material-based acoustic responses might not fully align with real-world acoustic properties, potentially impacting the system's reliability in applications requiring high fidelity in sound simulation.\par
The two presented acoustic material tagging methods primarily utilised a single sound rendering method to evaluate the system's performance in recognising different materials. This limits the understanding of how the system might perform across the diverse spectrum of sound rendering techniques. Sound rendering methods, each with unique strengths and handling of acoustic phenomena, could significantly influence the accuracy and efficiency of material recognition required. Expanding the testing to include a broader range of sound rendering methods would provide a more comprehensive assessment of the system's versatility and applicability in various simulation contexts.\par
The choice of deep learning architecture plays a pivotal role in the system's ability to recognise materials based on their acoustic signatures. Exploring alternative deep learning models, like self-supervised or unsupervised approaches \citep{gaur2019superpixel}, could provide more suited systems for capturing the subtle nuances of surface characteristics, enhancing the overall accuracy and adaptability of the tagging process. 
The training dataset used for the two systems plays a critical role in the development of any machine learning system. The texture-based acoustic material tagging system was trained on the OpenSurfaces~\citep{bell2013opensurfaces} dataset, which, while extensive, does not encompass the full variety of materials encountered in real-world or high-fidelity virtual environments. This limitation could hinder the system's ability to generalise across unseen materials or unique surface characteristics not represented in the dataset. Enriching the training dataset with a broader and more diverse collection of surface types and material properties would enhance the robustness and accuracy, enabling it to better adapt to a wider range of virtual simulation scenarios.

\subsection{Rendering Pipelines for Augmented Acoustics}
The architecture of the sound rendering system primarily relies on principles of geometrical acoustics, which, while providing a solid foundation for simulating sound propagation, introduces certain limitations. Geometrical acoustics often simplifies wave behaviors, neglecting complex phenomena such as diffraction and the nuanced interaction of sound with irregular surfaces. This simplification can lead to less accurate rendering in environments where these effects are significant, potentially affecting the immersive quality and realism in augmented reality applications, despite comparisons between real and virtual soundfields revealing acceptable error margins.\par
The efficacy of the sound rendering system in dynamic geometry scenarios requires further investigations around real-time uses by deploying an online sound renderer to a prototype. Furthermore, dynamic geometry, involving moving objects and changing environments, presents unique challenges for sound rendering, necessitating continuous data updates for accurate spatial audio. Testing an online sound renderer in \acrshort{ar} would address the responsiveness and adaptability in live dynamic environments, highlighting a crucial area for future development.\par
The testing conducted has predominantly focused on static sound sources, thereby not fully exploring the complexities introduced by non-static or moving sources due to moving sound sources requiring the deployment of a tested online sound propagation technique on a wearable computing platform These scenarios are critical for augmented reality, where sound sources often change position relative to the user. Additional testing is required to evaluate the system's ability to accurately render sound phenomena such as occlusion and reflection based on the dynamic positioning of sources and the geometry of the surrounding environment. Addressing this limitation is essential for achieving higher complexity in auditory interactions.\par
To date, the sound rendering system has not extensively explored the potential of deep learning-based approaches, which could offer significant advancements in the accuracy and realism of audio simulations. Deep learning algorithms have the potential to model complex acoustic phenomena more effectively than traditional methods, learning from vast datasets to predict sound behavior in varied environments. Further development and testing of these approaches are required to ascertain their feasibility and effectiveness in augmenting or replacing current sound rendering techniques, potentially leading to breakthroughs in how sound is simulated and experienced in augmented reality platforms.\par

\subsection{Psychoacoustic Evaluation}
A primary limitation of the investigation into the psychoacoustic effects of the sound rendering system is its restriction to data collected from a single environmental space. This constraint significantly impacts the generalisability of our findings, as acoustic characteristics and listener perceptions can vary widely across different spaces due to variations in size, geometry, and materials.\par
Future studies should aim to address generalisability and include a more diverse array of spaces, encompassing a range of architectural designs and acoustic properties. The use of a single sound rendering architecture limits the understanding of how different rendering approaches affect multimodal perception. Sound rendering systems vary in inputs/outputs and perceptual factors, each potentially offering distinct advantages or drawbacks in simulating accurate and believable soundscapes. Expanding future research to include multiple sound rendering architectures would provide a more comprehensive understanding of their psychoacoustic implications, allowing for a comparative analysis that could develop more effective and immersive audio solutions. A single and controlled environment was used due to the need for accessing the physical space to obtain ground truth \acrshortpl{ir}.\par
The current investigation has explored a minimal set of psychoacoustic abilities, leaving a spectrum of human auditory processing capabilities unexamined. Psychoacoustic phenomena such as sound localisation accuracy, auditory scene analysis, and the perception of sound quality under different conditions are critical for evaluating the effectiveness of sound rendering systems. Future investigations should aim to assess a broader range of psychoacoustic abilities to gain a deeper insight into the system's impact on the auditory experience and to identify areas where enhancements are needed to achieve a more realistic and engaging auditory simulation.\par
The study predominantly utilised static sound sources, neglecting the potential psychoacoustic effects introduced by dynamic and moving sound sources interacting with environmental geometry, such as occlusion. Sound sources' movement and interaction with the listener's environment can significantly affect spatial audio perception, offering a more complex and realistic auditory experience. Investigating the psychoacoustic effects with dynamic sound sources that exploit environmental geometry would provide insights into more realistic audio rendering and enhance the system's applicability to real-world scenarios with non-static sound sources.\par

\section{Future Work}
While this research has taken steps towards integrating sound rendering with computer vision to enhance immersive experiences, it has also uncovered numerous pathways for future investigations. The evolving landscape of technology and the increasing need for multimodal applications in \acrshort{xr} domains present both challenges and opportunities for advancing the state of rendering domains. Future work should aim to address the limitations encountered in the current study, considering scalability, real-time processing demands, and the incorporation of more complex environmental interactions. Specific areas where future research could not only extend the findings of this study but also open new avenues for innovation in sound rendering and computer vision are outlined here, aiming at the next generation of immersive technologies.

\subsection{Visual-Acoustic Mappings}
Future expansion points to the acoustic material tagging systems developed as part of the studies illustrated in Chapter~\ref{ch:materials} include experimenting with the use of unsupervised methods for material recognition tasks. Unsupervised approaches would constitute an alternative method to the two systems provided and allow improved workflows around training a model to recognise an ecosystem of acoustic materials given a set of virtual scenes. Unsupervised networks can create latent representations of acoustic materials as clusters of visual features extracted from the given set of scenes. The contribution of this expansion point would lie in the mappings between classes learned by extracting visual features via the unsupervised network and acoustic characteristics attributed to each class. Artistic control would be possible as a byproduct of this approach, allowing the network to specialise around selected environments and provide the ability to remap learned visual material classes to personalised acoustic characteristics. This would feed into procedural content creation, providing artists and worldbuilders with control over procedurally generated soundfields, given a set of representative environments.
An extension building from~\cite{Singh_2021_ICCV}'s would be an improved network considering listener and speaker position information in the mapping between RGB-D input and an approximated \acrshort{rir}. Instead, the fast image-to-image network would learn to manipulate existing \acrshortpl{ir} by using depth maps. The system would approximate how features of the environment expressed by the depth affect the response based on the listener and source position information encoded in the depth map data. The network would approximate propagation effects by leveraging systems like~\cite{chen2022soundspaces}'s, learning how an \acrshort{ir} is affected by the positioning of source and receiver and scene geometry. In the light domain, this is demonstrated by~\cite{wang2020deep} and \cite{gardner2019deep}'s methods, which estimate and manipulate light propagation effects depicted in RGB input image data, allowing rendering scenes with little knowledge of the virtual environment. This system would optimise the need for online acoustic rendering in \acrshort{ar}, as a generic \acrshort{rir} provided as input to the system would suffice for auralisations of multiple source-receiver position pairs.\par

\subsection{Acoustic Material Tagging}
By addressing the limiting factors of the acoustic material tagging systems discussed, ablation studies of deep learning-based systems would provide insights into the minimum requirements for a network to classify surfaces in complex scenes successfully.
Future work can expand this work and feed into cross-domain applications by intersecting elements of the proposed pipeline with reinforcement learning and robotics. This thesis work could be applied to autonomous audio-visual navigation in 3D environments, where agents are trained to learn to perceive and respond to audio-visual entities with the goal of navigating to a specific target. This demonstrates that a robust approach to retrieving acoustic information from virtual environments can benefit audiovisual-based navigation applications. Generalisable and robust acoustic material systems have a direct impact on training models in differentiating and resolving features of soundscapes thanks to the complex mapping between visual features of surfaces in virtual environments and acoustic features in audio from simulated soundscapes.\par
Reinforcement learning models could also open avenues towards novel systems for acoustic material tagging. With the increase in multimodal autonomous navigation systems, like the work from~\cite{chen2022soundspaces}, \acrshort{rl} agents can interact with the environment based upon auditory stimuli generated via acoustic rendering. Future work could explore the use of \acrshort{rl} agents to pain surfaces with acoustic material, optimising aspects of the soundfield. This work would contribute to architectural applications, where control over aspects of the soundfield is required as part of the building planning process, as discussed by~\cite{vorlander2015virtual}.\par

\subsection{Dynamic Geometry Handling for Augmented Audio Reality}
As discussed in Chapter~\ref{ch:ar-pipeline}, the environment mesh reconstructed by the \acrshort{hmd} may not always accurately match the physical space, leading to errors in the simulated soundfield and the resulting auditory stimuli. To address these challenges, future work could focus on the development and integration of mesh completion algorithms within the sound rendering pipeline. These algorithms would be designed to infer and reconstruct the unseen portions of the scene, enabling the system to generate a more complete and accurate environment mesh even before the user has fully explored the space. By predicting the likely structure and materials of the missing areas, such techniques could significantly reduce errors in soundfield simulation, leading to more precise and realistic auditory interactions.
In particular, the application of advanced computer vision techniques, such as those demonstrated by~\cite{cao2022monoscene}, which predict 3D scene geometry from a single monocular RGB image, could address the issue of reconstructing 3D environments from limited information, improving the accuracy of the input model for sound propagation. Future research could explore how these techniques could be leveraged to enhance sound rendering in augmented reality, particularly in predicting unexplored areas of the physical space around the user.

\subsection{Perceptual Response Analysis}
Chapter~\ref{ch:acoustic-rendering} conducts extensive testing on the acoustic rendering pipeline employed by the \acrshort{ar} prototype and analyses objective and subjective factors of simulated soundfields. However, some unexplored avenues could expand the testing conducted, providing generalised guidelines for future sound rendering methods. A crucial expansion point here is the definition of a minimum input to a rendering pipeline to obtain a perceptually valid response --- an investigation that could be performed both on \acrshort{ga} and deep learning-based acoustic rendering systems.\par
Future work should analyse the minimum simulation resolution required for \acrshort{ga} pipelines, such as the number of rays or the order, as well as study the disparity between the reconstructed environment and the real space with respect to perceptual factors. The study would provide insights into the margins of error before the listener starts noticing discrepancies in perceived realism evoked by the simulated auditory display.\par

\subsection{Further Psychoacoustic Analysis}
The significance of the findings obtained from conducting the psychoacoustic study illustrated by Chapter~\ref{ch:evaluation} has contributed towards creating guidelines for designing realistic interactions in \acrshort{ar}. When designing tasks in AR involving the application of psychoacoustic abilities, such as pinpointing the position of sound-emitting objects or resolving the nature of concurrent sound sources, acoustic modelling based on a virtual reconstruction of the space can suffice to affect task performance. From this practical implication stems the main theoretical contribution that transfers the established psychoacoustic characterisation of sound propagation effects to AR space, which has unlocked new potential in the realm of audio interactions. However, the theoretical contribution will require a set of complex scenes with diverse architectural and acoustic characteristics, evaluating the generalisability of the findings discussed in this work. Future work could benefit from the application of the testing methodology to a generalisable set of soundfields, including environments from high absorption and controlled acoustics, such as audio production rooms, to reverberant spaces like churches.\par

\section{Conclusions}
\subsection{Significance of Contributions}
One of the primary benefits of improved sound rendering in \acrshort{ar} through the proposed rendering pipeline is the enhancement of task performance. Accurate and realistic auditory information enables users to better interact with the immersive scene, leveraging sound cues to gain insights and make informed decisions \citep{rubio2017immersive,zimmons2003influence}. The integration of context-aware and spatially accurate audio allows users to perceive their environment more naturally, similar to how they would in the real world. This auditory augmentation supports multitasking, increases situational awareness, and enhances the ability to locate and identify objects within the \acrshort{ar} environment. 

\subsubsection{Impact on Modern AR Use Cases}
Examples of the impact of the thesis work on modern \acrshort{ar} applications include training and serious games where users can perform tasks. Generally, users can benefit from auditory information to guide actions and obtain feedback on task performance. Realistic spatialised audio generated via the proposed pipeline allows, additionally, the ability to discover visually hidden entities through means of psychoacoustics; e.g.\ via sound localisation. This enhances task performance to respond quickly and accurately to various situations, mirroring real-world conditions where sound-emitting entities play a critical role in situational awareness. This is essential in industrial \citep{machala2022application} or training \citep{soltani2020augmented}.\par
Some examples of this are in remote working environments, for instance, where team members are geographically dispersed, enhanced audio rendering can facilitate more natural and effective communication. Spatial audio can help users identify the direction and distance of users voices, making virtual meetings and collaborative work sessions more intuitive and engaging. Or in colocated working scenarios, where individuals share a physical space augmented with digital information, accurate sound rendering can support better coordination and interaction with shared virtual objects and information \citep{park2022metaverse,al2022review}.\par
In military and emergency response training, sound rendering can simulate realistic battlefield or disaster environments, where trainees must rely on auditory information to detect threats, locate teammates, or navigate through complex terrains. The ability to hear footsteps, weapon discharges, or environmental sounds can significantly enhance the realism and effectiveness of the training, complementing the lack or absence of visual information \citep{vine2023training, ahir2020application}.\par
In cultural heritage applications, \acrshort{xr} provides a platform to reconstruct historical soundscapes that are lost or decayed through time and damage. As discussed by Chapter~\ref{ch:ar-pipeline}, \acrshort{ar} \acrshort{hmd} provide technology to reconstruct physical space and project reconstructed and virtual geomtry to the user. Here, sound rendering allows historical changes applied to the virtual geometry to reconstruct lost soundscapes and provide users with experiences that reflect the orginal conditions of the historical site \citep{katz2020past, schofield2018viking}.

\subsection{Conclusive Remarks}
This thesis work builds on decades of advances toward systems for realistic auditory interactions for immersive media, adopting classic techniques for simulating soundfields and leveraging recent works within the domains of graphics and vision to optimise and automate several tasks associated with rendering believable auditory displays.\par
The primary goal of researching and proposing a novel system for \acrshort{ar} platforms was achieved, grounding its design choices on data collected from a series of experimental tests. Overall, the findings not only advance the understanding of sound rendering integrated with computer vision but also lay the groundwork for significant advances toward realistic auditory interactions in immersive virtual environments. This has implications not only for the entertainment industry, where enhanced immersive experiences can greatly elevate user engagement, but also for educational and training simulations, where realism can significantly improve learning outcomes and user task performance.\par
A conclusive remark that can serve the reader food for thought is the unexplored potential that can be leveraged by combining machine vision with acoustic rendering applied to immersive platforms. Finally, the potential for these technologies to assist visually impaired individuals by providing more intuitive and context-aware navigational aids represents a meaningful step towards technology that provides context-aware sonic interactions and realistic soundscapes.
