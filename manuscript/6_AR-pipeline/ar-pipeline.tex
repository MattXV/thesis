\chapter{Towards Scene-Aware Acoustic Rendering Pipelines for Augmented Audio Reality}\label{ch:ar-pipeline}% If you're changing this, update Section 1.6
The following Chapter details the implementation to address the primary objective of the thesis: proposing a scene-aware interactive rendering system for realistic sound transmissions between sound sources and a listener in AR space. The following Sections will detail how the acoustic material recognition and rendering prototypes integrate with the overarching pipeline, discussing target functionalities, design choices, limitations, and future expansions based on evaluation data obtained from evaluations provided in earlier Chapters and recent advances in the field. This Chapter breaks down into the following three areas:
\begin{itemize}
    \item \textbf{concept},
    \item \textbf{implementation}, and
    \item \textbf{vision}.
\end{itemize}
\emph{Concept} defines the proposed pipeline's architecture, objectives, and design principles; \emph{implementation} demonstrates how the pipeline can deploy to consumer HMDs as a proof-of-concept system; finally, \emph{vision} analyses limiting factors and expansion points, looking at perception factors to consider for optimisation and ablation studies.\par
Discussions on design principles, implementation, and limitations feed into methodologies for future research direction, as demonstrated by Chapter~\ref{ch:Evaluation}, where the evaluation design has foundations on the outcomes of these Sections.

\section{Concept}\label{sec:overview}
The proposed system aims to produce realistic sound transmissions between sound-emitting holograms in AR space and a listener experiencing the holograms via a head-mounted holographic display and perceived auditory stimuli via headphones. In this chapter, the term ``listener'' is used interchangeably with the term ``user'' to refer to the user who is displayed audio-visual stimuli from both the holographic HMD and the headphones. The implementation of this method for interactive sound rendering is demonstrated on a Microsoft Hololens 2 Augmented Reality Head-Mounted Headset, an embedded wearable computer with a holographic display featuring space-sensing and spatial mapping technology that enables Simultaneous Localisation and Mapping (SLAM) methods~\cite{davison2003real, ungureanu2020hololens}.\par

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{ar-system}
    \caption{Overview of the presented method showing a user (blue) and listener (red) in AR space: audio-visual stimuli are displayed to the user through headphones and a head-mounted display, experiencing virtual holograms projected onto his surroundings. The proposed system aims to render sound transmissions from sound-emitting holograms and the listener realistically.}
\label{fig:method-overview}
\end{figure}

\subsection{Dynamic Environment Reconstruction}
The first link in the chain of generating realistic sound transmission between a listener and holograms in AR space is generating and handling a virtual environment to host scene elements such as virtual sound-emitting entities. In AR space, the scene geometry includes a reconstruction of the real space surrounding the user. The space surrounding the user is sensed using space-sensing technology featured in the Hololens 2 HMD~\citep{ungureanu2020hololens}. Using the Microsoft Mixed Reality Toolkit~\footnote{https://github.com/microsoft/MixedRealityToolkit-Unity}, an environment mesh is continuously extracted as a triangulated mesh and updated every 3 to 5 seconds, reflecting dynamic changes in the environment like furniture being moved or a crowd entering the space.\par
The triangulated mesh, represented as a list of triangles and indices, is used to construct a Bounding Volume Hierarchy (BVH), where triangle primitives are wrapped in Axis-Aligned Bounding Boxes (AABBs), optimising geometrical search operations for acoustic modelling such as ray-triangle intersection tests. As discussed in Section~\ref{sec:bg-geometry-handling}, One key advantage of using a BVH in the pipeline is the ability to group multiple primitives in AABBs whose size can be determined by a geometry reduction algorithm to simplify the environment complexity whilst maintaining a target perceptual response from the acoustic model. \par
\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{bvh-matterport}
    \caption{A visualisation of a BVH constructed on a portion of a scanned environment, left image, to demonstrate how AABBs encapsulate triangle primitives, visible in the centre image, forming a tree of boxes to partition the geometry.}
\label{fig:bvh-visualisation}
\end{figure}
The BVH implementation supports traversing and optimisation operations, allowing the BVH tree representing the environment to support deformation and moving objects at interactive rates \citep{wald2007ray}.

\subsection{Material Tagging from Partitioned Space}
One key advantage of the employment of AABBs-based BVH for the handling of scene geometry is the space partitioning that can be used for the material tagging process. As discussed in Chapter~\ref{ch:Materials}, CNNs offer efficient approaches to mapping the appearance of surfaces to the acoustic characteristics of their materials, like absorption or scattering coefficients. The space partitioning provided by the BVH here is used to capture the appearance of portions of the reconstructed environment in order to determine the material characteristics of its surfaces.\par

\subsection{Acoustic Rendering Pipeline}
\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{ar-pipeline-diagram}
    \caption{An overview of the technical implementation of the proposed pipeline for spatial audio rendering applied to an Augmented Reality interactive application. }
    \label{fig:ar-pipeline-overview}
\end{figure}

\subsection{Overview}
The geometry handling system provides the foundations for supporting real-time auralisations in AR, as it provides a solution to the problem of defining acoustic geometry: auralisations can be produced from simplistic cuboid volume to realistic reconstructions of space with orders of fidelity to the physical counterparts of millimetres. Computational costs associated with generating auralisations often rise with the number of geometrical primitives used in the reconstructed space: in triangulated meshes, detailing smaller structures in a 3D model increases the number of triangles. Detailed scene geometry allows approximation of acoustic phenomena and generally contributes to high-resolution soundfield modelling, though the computational requirements make it unfeasible for interactive applications. As discussed in Chapter~\ref{ch:litReview}~and~\ref{ch:acousticrendering}, studies outline a perceptual threshold and benchmark to define the minimum level of detail required from reconstructions, and recent research on acoustic simulations conducted over the last decades has expanded towards defining what is required from a virtual environment to produce a believable simulation. Combined with advances in real-space scanning technology and user-friendly 3D reconstruction software, it is now possible to create appropriate virtual environments for acoustic simulations without requiring expert computer graphics engineering knowledge.\par

\subsection{Selecting Acoustic Rendering Techniques}
A central problem inherent in the task of interactive soundfield approximations for generating dynamic auralisations is the task of adopting a method for propagating anechoic audio in a dynamic scene. As discussed in Chapter~\ref{ch:Background} there are families of sound rendering techniques that produce auralisations with varying levels of realism evoked by audio propagated in the simulated soundscape and associated computational costs, as well as limitations due to their inherent architecture or nature.

Ray-based techniques, wave-ray hybrid techniques, and wave-based techniques have emerged as prominent methods to generate impulse responses in the field of acoustics, each contributing to understanding how sound propagates within a given space. Ray-based techniques, rooted in geometric acoustics, simulate sound by tracing rays that emanate from a source and bounce off various surfaces within a space. This method creates echoes and reverberations, contributing to the overall impulse response. Notably, it offers efficiency as it is computationally less demanding than wave-based techniques, making it suitable for real-time applications or large-scale spaces often found in cultural heritage contexts. The geometrical nature of ray-based methods allows for easier integration with existing architectural models or historical reconstructions, and the method's inherent flexibility makes it easily adjustable to different acoustic scenarios. IS THIS OK FOR DYNAMIC AURALISATIONS

On the other hand, wave-ray hybrid techniques present a more complex picture, combining aspects of ray-based and wave-based methods. Rays are utilised to model the high-frequency components of the sound, while wave equations handle the low-frequency behaviour, attempting to capture the best attributes of both methods. However, the hybrid nature often means more computational resources are needed, and it might not always be the most suitable choice for cultural heritage applications where both high and low-frequency accuracy is not often the primary concern. IS THIS OK?

Wave-based techniques stand out for their precision, solving the wave equation to simulate how sound waves propagate through a space, accurately modelling diffraction, scattering, and other complex wave phenomena. While highly accurate, wave-based techniques often require substantial computational resources, making them less suited for real-time or large-scale applications. Moreover, this level of detail may exceed what is necessary for conveying the historical or cultural experience.

Considering the landscape of immersive acoustics, ray-based techniques offer a compelling option for cultural heritage contexts. Their computational efficiency, relative simplicity, and adaptability in handling various scenarios effectively capture the essential acoustic characteristics of historical spaces. Unlike wave-based or hybrid methods, ray-based techniques can prioritise the aspects most relevant to the experience and understanding of cultural heritage, aligning well with the objectives and constraints often found in this field. Therefore, while the high accuracy of wave-based methods or the comprehensive nature of hybrid methods may have specific applications in areas of cultural heritage investigations, it is the ray-based techniques (such as those employed by acoustic simulation software such as ODEON) that generally stand out as the most appropriate choice for the unique challenges and opportunities presented within the context of real-time applications.

\subsection{Real-Time Spatial Audio Rendering Techniques}
The proposed system approximates acoustic phenomena from a dynamic environment reconstructed as the user interacts and navigates the complex scene, considering physical and objective factors of the soundscape. The HAS, however, is the final and crucial link in the chain of the reproduction system, requiring the pipeline to consider psychoacoustic and human factors in generating auditory stimuli expressed as binaural audio. The integration of factors of the HAS and the convolution of anechoic audio completes the 3D spatial audio chain.\par
Head-Related Impulse Responses (HRIRs) offer a compact solution to the integration of HAS features, much like room responses, as we can convolve a monaural signal to the frequency representation of a pair of HRIRs (one for each ear), the Head-Related Transfer Function (HRTFs). Hence, the discussed real-time convolution algorithms [REF] can be used, in combination with the $h$ RIR signal, to generate binaural rendered audio channels $y_{left}$ and $y_{right}$ from a monaural signal $x$:
\begin{equation}
    y_{left} = x * HRTF_{left} * h \quad \textrm{and} \quad  y_{right} = x * HRTF_{right} * h \textrm{.}
\end{equation}
HRTF channels left and right are evaluated in real-time, depending on the current position and rotation of the listener entity in the virtual environment, using a loaded bank or HRIRs that are often real measurements of subjects across a grid of points in the azimuth and elevation surrounding the subject. These measurements record the response of each ear canal from a source position in the surrounding sphere, allowing an interactive HRIR algorithm to interpolate between these measurements. A range of objective metrics are commonly used in acoustic measurements. These metrics offer an understanding of acoustic properties of spaces, such as reverberation, clarity, strength, directionality, and intelligibility, and can apply to various contexts, from concert halls to cultural heritage sites. 

\section{Implementation of an AAR Prototype System}
The following Section provides a software implementation of the system, proposing a technical apparatus that can be employed for dynamic auralisation of sound-emitting holograms, considering approximated acoustic phenomena of AR space.

\subsection{Overview}
The system, illustrated in Figure~\ref{fig:ar-pipeline-overview}, has a game engine as its core component, responsible for querying the HMD for updates on the spare surrounding the user, indexing and handling reconstructed scene geometry, computing energy transfers between emitters and receivers, and controlling a DSP engine for performing audio rendering tasks.
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\linewidth]{source-flow-diagram}
    \caption{Rendering process of a single sound source: an input source description, positioned in AR space, is used to model its energy transfer to the listener, represented via a response function. Via head-related transfer functions based on the listener's head rotation provided by the HMD, audio from the audio is processed and sent to the output audio buffer.}
\label{fig:source-flow-diagram}
\end{figure}

\subsection{Hardware Apparatus}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{hl2-exploded-view}
    \caption{Exploded view of the Microsoft Hololens 2 Augmented Reality Head-Mounted-Display.}
\label{fig:hl2-exploded-view}
\end{figure}
This Section demonstrates the system implementation using a Hololens 2\footnote{\href{learn.microsoft.com/en-us/style-guide/developer-content/reference-documentation}{https://learn.microsoft.com/en-us/style-guide/developer-content/reference-documentation}} HMD, composed of a built-in ARM32/64 computer equipped powered by a Qualcomm Snapdragon 850 CPU with 4GB LPDDR4 DRAM.\par
As shown in Figure~\ref{fig:hl2-exploded-view}, the HMD has a visor containing holographic visual displays and featuring head-tracking sensors with four light cameras, two eye-tracking infrared cameras, 1 Megapixel depth sensor. The visual displays have $1.08mm$ focal length and $96.1^\circ$ field of view. Additionally, an accelerometer, a gyroscope, and a magnetometer allow tracking of the user's position and 
orientation. The device supports spatial audio reproduction with built-in speakers and a Bluetooth-connected external audio DAC. The array of sensors enables several human understanding features, such as hand and eye tracking and six-degrees-of-freedom position tracking.

\subsection{Game Engine and Scene Management System}
Sound sources in AR scene. Holograms. Audio source description. Sound synthesis.

\subsection{DSP Engine for Spatial Audio Rendering}
Drawing from well-established and modern spatialiser system designs from~\cite{naef2002spatialized}~and~\cite{lakka2021x3d}, respectively, a DSP engine receives control from the game engine, transforming the energy propagation modelling into binaural IRs that can propagate anechoic audio. Duties of the DSP engine include manipulating signals in the time and frequency domain, requiring a fast implementation of the DTFT, and a suite of tools and routines for handling and processing IRs and transfer functions.\par
As shown in Figure~\ref{fig:source-flow-diagram}, input audio from sources undergo time and frequency-domain manipulations based on aspects of the environment and the listener implemented by the DSP engine. Table~\ref{tab:dsptb-interface} illustrates the interface to the DSP Engine, showing procedures and set-up operations to support real-time audio manipulation of sources. The set-up phase determines the sample rate for all audio manipulations, the number of points for evaluating kernel functions for filters, and the definition of frequency bands for frequency-dependent energy transfer modelling and processing. In addition, other set-up procedures include loading HRTF banks from a participant record database and configuring block processing routines by specifying the current engine's audio buffer properties, such as the size of audio chunks and number of channels \citep{hoene2017mysofa}.\par
Runtime procedures, executed as the game engine runs the scene and a sound source is emitting with the listener being within the propagation radius, include setting frequency-dependent energy transfer between source and receiver, rendering a monoaural IR, and processing audio emitted based on the listener's head rotation; these operations complete the chain in Figure~\ref{fig:source-flow-diagram}.\par

\input{dsptb-interface}

\subsection{Interactive Acoustic Rendering}
At runtime, the game engine approximates energy transfer between a source and the listener, generating frequency-dependent energy histograms using a ray tracer. The ray tracer implementation follows the method provided in Section~\ref{sec:ga-based-pipeline}, underpinned by~\cite{schroder2011physically}'s work in physically-based acoustic rendering.
Given a source-receiver pair, the game engine can generate propagation a set of propagation paths for each defined frequency band, which are defined based on frequency-dependent material properties assigned to the scene geometry.\par

\subsection{Audio Rendering}
Approximated acoustic phenomena expressed in IRs are often applied to anechoic, unpropagated audio by application of the convolution operation, as discussed in Section~\ref{sec:real-time-conv}. The resulting auralisation targets a sound reproduction system by applying responses to model characteristics of a human listener in the sound field.\par
Audio rendering describes operations related to such tasks, requiring DSP algorithms for real-time auralisation of audio signals from sources in a complex scene. The apparatus can display audio stimuli as a stereo signal via headphones connected to the master bus output of the game engine. The audio rendering implementation for the system needs to consider modelled source-receiver acoustic characteristics and apply spatialisation effects expressed as HRTF, inserting a spatialised system in the sound reproduction chain \citep{liu2022sound}.\par
Real-time acoustic effects apply to audio buffers from scene sources via real-time DSP convolution algorithms such as Overlap-Save or Overlap-Add. The former optimises the number of convolution operations. Though this is unfavourable when the kernel function is updated frequently, leading to audio artefacts caused by outdated convolutions applied to the audio buffer. Hence, the implementation adopts the Overlap-Add algorithm, allowing for fast convolution of kernel functions, which can be updated across update calls within the engine audio thread to audio buffer chunks.\par
Figure~\ref{fig:overlap-add} shows the algorithm implementation considering HRTFs combined with RIR applied to a given audio signal block from the audio buffer associated with a given scene source, showing the system as a single channel for simplicity. For each update call in the engine audio thread, the algorithm convolves a block of audio samples of length $B$ containing interleaved stereo signal to a kernel function of length $K$. Zero padding, applied to both the kernel function and blocks, allows a DFT toolbox to multiply the spectrum of the two signals ($\bigotimes$ operation), generating a $B+K-1$ convolution result. $B$-long blocks (the turquoise blocks) from past convolution results sum into a new block, discarding already summed blocks (the gray blocks). Finally, the new block replaces the given input block from the audio buffer.\par

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{overlap-add}
    \caption{Overlap-Add algorithm used for real-time convolution of a kernel function expressing Room Impulse Responses and Head-Related Transfer Functions applied to chunks of audio data from an audio buffer.}
\label{fig:overlap-add}
\end{figure}

\subsection{Acoustic Geometry Handling}
Syncing and updating a server-side geometry handling system using a simplified mesh, to which material recognition procedures can also be offloaded.

\section{Vision for Dynamic Auralisation Systems}
This section outlines the design of future prototypes for dynamic auralisations expanding from the proposed systems toward real-time interpolation.\par
Section~\ref{ch:Evaluation} demonstrates a prototype deployment of the implementation proposed in the previous Section, characterising the psychoacoustic factors associated with approximating the soundfield surrounding the user. The implementation focuses on a single sound source, omitting optimisations that target multiple sources, real-time aspects, dynamic environments scenarios, and factors targeting realistic acoustic rendering.\par
With the human listener being the target for the generated audio-visual stimuli, rendering procedures affecting the perceived quality require probing of human perception to profile realism and performance aspects of the system. 

\subsection{Psychoacoustic Characterisation of Acoustic Rendering}
A crucial aspect associated with the vision of a real-time system is defining the need to outline the minimum required threshold for realism in IRs.

\subsection{Extension to multiple sources and source clustering}
Source clustering based on learned psychoacoustic threshold.

\subsection{Extension to Dynamic Impulse Response Interpolation}
learning a compact image2image to process 

\subsection{Extension to Radiance Fields for Inferring Local Acoustic Features of Reconstructed Space}

BVH space handling with image2reverb projections
local feature query via neural radiance field
Even with radiance fields, the interface between the game engine and the DSP engine would be maintained valid. The game engine would query the radiance field based on the current source and listener position.
