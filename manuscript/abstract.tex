\beforeabstract{}
\prefacesection{Abstract}
Immersive technology permeates an increasing number of industry domains, causing a wide range of applications to benefit from visualising, experiencing, and manipulating digital reconstructions of the physical world through the lenses of a Head-Mounted Display (HMD). Augmented Reality (AR), as an emerging branch of immersive technology allows the projection of visual and audio stimuli onto the users' surroundings, enabling new interaction paradigms that find applications in a broad range of use cases, like training and simulation, for medical or military domains, as well as manufacturing, cultural heritage, accessibility, and more.
The adoption of AR across these domains has incentivised the development of HMD technology and better and more optimal techniques for efficient and realistic holographic displays, leveraging sensing and capturing capabilities provided by the HMD. As a result, holograms can interact with spatial features of the users' surroundings, allowing for realistic context-aware interactions, and improving task performance and the perceptual quality of the overall immersive experience. However, techniques for rendering realistic audio stimuli that respond to spatial features of the immersive environment are underrepresented, considering a body of literature on Extended Reality research domains. 
Physically based sound rendering is key to achieving realism in audio stimuli within immersive environments, as spatial features of the users' surroundings can be considered, simulating basic characteristics of sound propagation in environments. This enables listeners to use natural hearing abilities that interpret sound propagation effects to sense space and entities in their proximities, affecting activities and interactions in immersive experiences. There is a large family of techniques for simulating sound propagation effects within virtual environments, building from decades of research that provide a variety of techniques with varying degrees of realism and computational requirements.
In the course of this thesis, the current state of sound rendering techniques is reviewed, discussing their application and feasibility for AR use cases, aiming at proposing a novel pipeline that can generate context-aware realistic audio for AR applications. The development of this pipeline involves adopting computer vision techniques in the process of decomposing complex scenes to recognise acoustic characteristics of space, determining physical and structural features of complex scenes, and allowing audio stimuli to respond to spatial characteristics of the immersive environment.
The experiments presented demonstrate applications of scene understanding techniques to game scenes and virtual reconstructions of real space to determine acoustic properties of scene geometry for automating realistic sound rendering, identifying the current state of automatic acoustic material recognition for virtual environments and proposing a novel evaluation framework to test objective and subjective accuracy against measurements from real environments. Proof-of-concept systems have been tested on state-of-the-art acoustic renderers to demonstrate their efficiency in offline procedures. Subjective testing conducted using a prototype deployment of the proposed pipeline suggests that audio stimuli generated using the proposed pipeline have a significant effect on task performance within AR.
Current directions are aimed at designing end-to-end pipelines for interactive, real-time applications, with the ambition of adopting computer vision to understand the acoustic space, even in contexts of dynamic geometry typical of AR platforms, where the acoustic space is constantly updating based on the surrounding, real world.

\prefacesection{Preface}
The Acknowledgements section may be used to thank your supervisor, family, research funding bodies, or any other applicable individuals or institutions.

\afterpreface{} \afterabstract{resources/signature_mattia_colombo}
