\chapter{Advances in Visual-Acoustic Mapping Methods and Sound Rendering Pipelines}\label{ch:lit-review}% TODO Introduce each area properly
In light of the grounding provided around the domains of wave theory, digital signal processing, acoustics and soundfield simulations, as well as domains of computer graphics and vision, virtual environments, and immersive displays, this Chapter reviews the current state of research intersecting the primary aim of the thesis. Throughout this work, Sections of this thesis may re-iterate objectives to provide the reader with context relative to the Chapter or Section at hand.\par
As the overarching goal of this work is to explore the potential of computer vision within acoustic rendering pipelines for realistic sound transmissions between virtual sound-emitting objects perceived by a user in a virtual environment, experimental methods and novel systems are reviewed. The engineering process supporting the overarching aim addresses a set of problems arising from various facets of the sound rendering pipeline. With the major problems being addressed over the development of individual components of the system, this Chapter discusses how recent work has developed similar pipelines for tasks around visual-acoustic matching problems, auralisations, or sound rendering pipeline designs.\par
Advances in neural computing and computer vision are providing researchers with increasingly powerful and generalisable tools to address sound rendering tasks, generating an overwhelming volume of new research and novel system. Although every effort is being made towards reviewing cutting-edge and state-of-the-art work on these domains, claims or information provided on techniques might become out of date or inaccurate at the time of reading this work. The following Sections gather pioneering work, state-of-the-art, and experimental methods to address problems or tasks associated with each component of the thesis aim. Methods and experiments are reviewed, considering limitations and expansion points to inform design choices in engineering systems proposed throughout the following Chapters. Discussions around existing work aim at both orienting the reader towards the goal of each domain associated with the thesis component and defining the value of the contributions stemming from this work.\par

\section{Introduction}
The general trends of computing head towards ubiquitous \acrshortpl{ve} with increasingly realistic and interactive multimodal interactions \citep{al2022review, slater2009visual, park2022metaverse, rubio2017immersive}. Wider industry domains are exploiting the potential generated by recent advances in graphics, game engines, acoustic rendering and neural computing. In industry applications, \acrshortpl{ve} are often experienced via immersive and interactive, such as \acrshort{xr} platforms, incentivising manufacturers of \acrshortpl{hmd} to accelerate the development of wearable computing platforms, equipping them with better sensing technology and more accurate interaction apparatuses.\par

\subsection{Current Trends of Interactions within Immersive Platforms}
The domain of immersive acoustic has recently attracted more popularity thanks to the drive of \acrshort{xr} platforms towards better and more efficient multimodal interactions. Over the last couple of decades have seen an increase in experiments towards interaction, rendering, visualisation and related fields generating major research interests \citep{kim2018revisiting}.\par
Interaction techniques have seen an increasing number of experiments around immersive tasks performed by users within \acrshort{xr} technology. The range of applications employing \acrshortpl{hmd} across domains of research and industry requires virtual interactions to simulate human-to-human interactions in their realism and completeness. The spectrum of interaction techniques reviewed by~\cite{spittle2022review} shows that auditory interactions, such as speech input systems or language processing techniques are widely adopted to alter or manipulate virtual scene elements.\par
Auditory interactions have generated various subdomains in \acrshort{xr} domains with the aim of overcoming obstacles caused by factors influencing the relationship between aural cues and acoustic characteristics of the environment \citep{park2022metaverse}.
More specifically,~\cite{yang2022audio} have reviewed a body of literature around experiments towards auditory displays in augmented reality, forging the term \acrfull{aar}. The authors review research focusing on auditory interactions in \acrshort{ar} outlining crucial research problems affecting task performance, realism, presence or subjective factors associated with human perception. Some example research problems relating to the overall immersive experience revolve around acoustic factors of sound transmissions between physical or virtual entities and the user as a listener. For instance,~\cite{mansour2021speech} show that speech intelligibility in immersive environments perceived through ambisonics displays is a problem affecting \acrshort{xr} in noisy soundscapes and hindering accessibility for users with hearing impairments. Such problems incentivise the field of \acrshort{aar} to develop audio pipelines considering and compensating for acoustic factors of the environment.\par

\section{Review of Sound Rendering Pipelines for Immersive Environments}\label{sec:lr-visual-acoustic-mapping}
\cite{naef2002spatialized} present a novel architecture for spatialised audio rendering for virtual environments experienced through immersive headsets. They define 3D sound localisation, room simulation, live audio input and efficiency as the main requirements the architecture should feature. The architecture draws from low-level rendering pipelines, such as graphic sub-systems, to integrate sound rendering procedures into existing scene-handling systems adopted by these pipelines. 

\subsection{Advances in Sound Rendering}
Methods have been proposed to map visual representations of environments to their acoustic features. Sound rendering in virtual environments can leverage such mapping for producing audio stimuli conveying spatial information to the user. Recent work solves tasks within sound rendering for virtual environments, such as propagating audio within virtual environments.\par
Spatial sound has a significant effect on the sense of presence and immersion for a user in a \acrshort{ve} \citep{poeschl13}. Factors of accurate and plausible acoustic rendering include geometry, material definitions and a room impulse response which describes the attenuation of sound from a sound source to a listener, and there exist approaches that tackle varying aspects of these factors. A common denominator in the sound rendering methods mentioned is the problem of mapping the visual representation of environments to corresponding acoustic materials, which intersects image processing and computer vision domains aimed at modelling how human vision recognise materials.\par%
Acoustic rendering can reproduce spatial hearing abilities \citep{lokki2005navigation}, supporting architectural acoustics, cultural heritage \citep{berardi2016acoustic, vorlander2015virtual}, and computer games \citep{raghuvanshi2014parametric, mehra2015wave} to build compelling, realistic acoustic simulations. Recent advances in wavefield synthesis have made it easier and computationally feasible to apply to \acrshortpl{ve}~\citep{raghuvanshi2014parametric}. They draw on geometrical acoustics, wave-based or hybrid sound propagation algorithms, simulating sound propagation by tracing rays or beams \citep{hulusic2012acoustic}; solving the wave equations at discretised junctures of the representation of the environment or by a combination of the former techniques. These techniques enable virtual complex scene designers to apply realistic, spatialised audio to immersive applications and are becoming part of standardised workflows in game engines.\par
In acoustics, it is common to capture an environment adopting measurement techniques such as the sine sweep, usually consisting of reproducing a logarithm sine chirp or a short burst, e.g., a gunshot,  emulating a Dirac-delta function to excite frequencies in the audible spectrum and recording how the environment influenced the propagated sound at the listener position \citep{reilly1995convolution}. Such measurements can determine a Room Impulse Response (RIR), a series of reflection paths over time, recreating the acoustic space for a given source-listener position pair. Wave-based acoustic simulations achieve the highest degrees of realism in generating acoustic fields as they compute sound propagation via simulations of high-dimensional pressure fields \citep{raghuvanshi2014parametric} or solving the wave equation with Finite-Difference Time-Domain schemes \citep{hamilton2017fdtd}. Their inherently complex nature requires solving the wave equation to produce acoustic simulations for a given scene, and despite recent GPU-based solvers optimising complexity by orders of magnitude \citep{mehra2012efficient}. Their computational requirements are often impractical for real-time applications due to the nature of the wave equation, resulting in numerical complexity increases with frequency. On the other end of the spectrum, Geometrical Acoustics (GA) provide methods for fast approximations of acoustic space; they have gained popularity among extended reality platforms due to their highly parallelisable implementations \citep{savioja2015overview}.\par
\cite{schissler2016interactive} introduced an acoustic rendering system based on ray-tracing, adapting to large complex scenes. Among their contributions is overcoming the problem of handling many sound sources in large-scale environments by clustering them based on the distance from the listener. Based on an octree representation of space, with respect to the listener position, their clustering aggregates increasing numbers of sources as their distance from the listener increases. Their approach highlights the need for dissecting the acoustic space for efficient selective rendering, resulting in rendering of fine perceptual details within the listener's close proximity and coarse approximations otherwise.\par

\cite{schissler2021fast} recently presented a novel method for computing acoustic diffraction in real-time, which can adapt to GA frameworks. They target complex scenes typical of virtual and augmented reality applications. Their approach can overcome the shortcoming of \acrshort{ga} techniques in approximating soundscapes, thanks to the ability to incorporate simulated propagation effects into their proposed sound rendering pipeline. The main contribution of their work is a mesh processing system that optimises diffraction simulations for environments expressed through dense geometry. Results gathered from their evaluation show that the technique is comparable to \acrshort{fdtd} methods, obtaining a high degree of realism from generated propagation data. Due to the novelty and recency, there is a lack of psychoacoustic characterisation performed on their method, making it hard to \par

\subsection{Differentiable Methods for Sound Rendering}
\cite{manocha2020differentiable} present a model for simulating sound fields using neural networks without pre-computing the wave field of an acoustic environment, predicting unseen objects with arbitrary shapes in a VE for sound propagation at interactive rates. They train a geometrical neural network on annotated meshes to infer acoustic data associated with the represented object.
\cite{chen2022visual} introduce a novel task dependant on this mapping, \textit{visual/acoustic matching}, which produces acoustic stimuli responding to a target space depicted in an image, given an input audio excerpt and an image of the environment in which excerpt propagated. The rapid development in DNN for multi-modal applications has opened new avenues in the field of sound propagation modelling, one of which tapped into visual-acoustic mapping, the process of determining relationships between visual and auditory features in audio-visual or immersive media.\par
One innovative experimental method,~\cite{Singh_2021_ICCV}'s work into Image2Reverb, ventured into using \ACRshortpl{dnn} to define mappings between images and reverberation, expressed as an \acrshort{ir}. By observing photographs of real or virtual environments, our visual system is generally able to infer acoustic characteristics of the space; from a photograph of a cathedral, for instance, we can imagine its reverberant aural footprint. The authors leverage \acrshortpl{gan} to explore automated mappings between deep visual features extracted from a given input image, representing an environment, and an output spectrogram of an \acrshort{ir}. Since many reverb metrics like the $T_{60}$ are linearly correlated to the energy decay in \acrshortpl{rir}, their network encodes reverb by representing a spectrogram with variable energy decay. The authors train and evaluate the network by comparing results to ground truths pairs of photographs and measured responses, achieving around $0.87s$ mean error in $T_{60}$ estimations.\par
Improvements and new approaches to solving the task are being explored at increasing rates, such as~\cite{somayazulu2023self}'s network presenting a self-supervised visual-acoustic matching system. With an input audio excerpt and a target image representing an environment, their system re-synthesises the audio excerpt to reflect the acoustic features of the target environment. A key novel aspect of their method is the handling of reverberant audio by leveraging a state-of-the-art network for audio dereverberation (which is a well-established task in the field of acoustic signal processing). The de-reverberated audio is passed to a GAN, which optimises the output audio until it acoustically matches the extracted visual features.\par

\cite{liang2023neural} present a method that improves on the adoption of Neural Radiance Fields for sound propagation. The authors present a method that allows a neural field to be learned on a real soundscape by providing emitter and receiver position input, a ground truth RIR, and acoustic context representations. The neural field fit on the input environment allows the generation of novel RIRs based on given emitter-receiver position pairs. The neural field learns from multimodal representations of the environment, expressed as visual information by RGB + depth images and acoustic data by emitter-receiver position information.\par

\cite{tang2020scene} present a novel scene-aware sound rendering system aimed at rendering audio considering acoustic characteristics of a given room, providing real-time audio effects applied to novel signal matching the soundscape of the input room. The system uses a neural network to infer reverberation time and estimate resonance interferences caused by the room architecture using a recorded signal and 3D representation of the environment where the recording is generated. Their method uses the inferred acoustic properties as input to an acoustic simulator that generates and optimises acoustic materials by measuring simulation errors against the estimated room features. Once the acoustic simulator optimises materials, it convolves IRs with novel audio signals to emulate a sound source propagating in the input environment. As part of the testing procedures, the authors provide a benchmark for the material optimisation pipeline, outlining the error in estimated materials across rooms of increasing reverberation times; it increases with the size of the input room. Their system provides realistic acoustic stimuli as subjective tests show that the simulation error is not perceptually significant.\par

\cite{chen2023everywhere} use Audio-Visual receivers to sample reference features, generating joint audio-visual representations of input scenes to synthesise novel binaural audio. Their system takes visual information and uses a Joint Audio-Visual Representation to extract audio-visual features from space, which feed into an Integrated Rendering Head. The rendering head uses a ground-truth binaural waveform to optimise output binaural audio generated given a listener position. Their rendering pipeline improves state-of-the-art methods, such as few-shots learning-based techniques for sound rendering, by evaluating simulations on standard acoustic scenes and indoor space reconstruction datasets.\par

\cite{ratnarajah2022mesh2ir} present a pioneering approach to neural networks for sound rendering as an alternative to physics-based \acrshort{ir} computation methods like geometrical acoustics or wave-based methods. The core task of their method is to match auditory stimuli with visuals of a \acrshort{ve} for applications around audio-visual navigation, auralisations, speech enhancement, dereverberation and more.\par
Their method takes a triangulated mesh representing the environment, which is fed into a series of graph \acrshort{nn} to process vertex and edge information to create a graph encoding of the input scene, simplifying topology information. A modified \acrfull{gan} uses the constructed graph representation to generate an \acrshort{ir} by computing a decay curve and optimising acoustic characteristics encoded in the graph representation using a generator and discriminator. Their method is tested on indoor scenes, evaluating $T_{60}$ reverberation, direct-to-reverberant ratio and early decay times. The evaluation shows less than $10\%$ error across all metrics, placing the method amongst one of the pioneering \acrshort{nn}-based approaches.\par

\cite{yang2020fast} present a method for synthesising \acrshortpl{rir}, reproducing perceptually-convincing acoustics of real environments based on a small number of ultrasonic measurements. The method consists of using a loudspeaker and a microphone to record an ultrasonic \acrshortpl{ir} that can be transformed into an octave-IRs by approximating reflection decay curves. Octave-IRs constructed from Gaussian noise and modelled using estimated decay curves are combined into a final monoaural response. The authors test the approach in two indoor spaces (a lounge and a classroom) demonstrating that the technique can generate perceptually plausible auditory stimuli and showing potential application for \acrshort{ar} platforms. However, the apparatus adopted would require the wearable \acrshort{ar} device to be equipped with a recording setup to sample ultrasonic responses.\par

\cite{li2018scene} identify a novel method for acoustic simulations using convolutional neural networks to perform acoustic analysis on videos, veering away from more formal 3D scene definition. This approach synthesises \acrshortpl{rir} for environments' representations from audio-visual scenes. Their system extracts high-level acoustic properties such as reverberation time T\textsubscript{60} and frequency-dependent amplitude level equaliser.\par

\subsection{Discussion}\label{sec:sound-rendering-advances-discussion}
The spectrum of sound rendering techniques continues to refine existing methods, increasing their efficiency and applying them to newer platforms and use cases, as well as present novel methods leveraging recent advances in deep learning. Table~\ref{tab:visual-acoustic-mapping} summarises the crucial techniques, reporting advantages and limitations in relation to their employment in real-time auralisation applications.\par
A common shortcoming of experimental and novel methods for interactive sound rendering lies in the limited testing and benchmarking conducted on the techniques. Such a shortcoming does not necessarily invalidate the value of the contributions or their potential for real-time application in \acrshort{xr} domains though provides research directions for future work. Although deep learning approaches are capable of generating auralisations at interactive rates, there are still challenges along the avenues of applying them to wearable computers like \acrshort{hmd} due to the computational requirements. Given the current state of research towards deep learning-based techniques, their deployment to \acrshort{ar} platforms would require specialised hardware, such as \acrshortpl{tpu}. Cloud computing alternatives could also facilitate the deployment of deep learning models, requesting inference from \acrshortpl{hmd}.\par
\acrshort{ga}-derived methods, such as~\cite{schissler2016interactive,saviojaGA,schroder2011physically}'s, share some inherent limitation of the wider geometrical acoustics family; though there is a deeper understanding of their perceptual impact and are generally easier to scale for platforms with limited computational resources.
\input{visual-acoustic-mapping}

\section{Material Recognition for Rendering Tasks}
Material recognition for rendering pipelines is a generally narrow research domain with a niche application, and there is a limited body of literature and development toward solutions. This niche technique derives from the thriving and popular superset of literature on the recognition and understanding of material information, benefitting from various advances in deep learning methods for understanding, classification, or detection tasks. Reviewing the superset of techniques is outside the scope of this work; hence, this Section focuses on applications to rendering tasks, discussing the relevance of novel techniques and their shortcomings in relation to the relevant components of this work.\par
Considering works related to the design of a system for extracting material information from \acrshortpl{ve}, techniques are categorised into supervised and unsupervised algorithms. 

\subsection{Supervised Material Recognition Techniques}
\cite{schissler2017acoustic} present a two-stage system for sound rendering based on scene understanding performance on scans of physical space, requiring reconstruction of physical space and acoustic measurements as input and, leveraging recent advances in semantic segmentation for audio-visual rendering tasks. The first stage of the system uses multiple camera viewpoints to reconstruct a dense 3D triangle mesh representing the environment and generate input to a CNN to classify acoustic materials from camera renders. A Least Square Solver algorithm uses real measurements to optimise the inferred materials by calculating the distance from estimated IRs to the ground-truth IRs.\par
Semantic segmentation tasks aim to assign a semantic class label to every pixel in the input image. Examples of applications in scene understanding include PixelNet \citep{bansal2016pixelnet}, which performs semantic segmentation and edge detection; EdgeNet \citep{dourado2019edgenet}, which combines depth information with semantic scene completion, using RGB-D input data. For synthetic data generation, UnrealCV provides a pipeline that generates images from VEs providing semantic segmentations \citep{qiu2016unrealcv}, allowing for easy generation of training data.\par
Large-scale datasets, including semantic and 3D information, have been released, e.g. the Matterport3D dataset \citep{chang2017matterport3d}, which provides panoramic images generated across real environments. Various domain-specific applications of these methods have been proposed, e.g. in mixed and augmented reality \citep{chen2018context}, where semantic information about surfaces can guide contextual interactions between virtual elements and real-world structures; or surveillance \citep{mao2018aic2018}, where the semantics of objects in the scene determine its subsequent processing.\par
However, few examples of applying computer vision to realistic audio rendering exist. \cite{kim2019immersive}, as an alternative approach, adopts $360^{\circ}$ photographs and depth estimates to generate 3D geometry and semantic information, which is then used for physically-based audio rendering and can also adapt to VEs. In this context, even approximate semantic information could allow for gains in efficiency and a decrease in the costs of applying physically-based audio rendering to VEs.\par
Recent developments in deep learning techniques have contributed to a dramatic increase in accuracy in tasks such as image classification. Specifically, convolutional neural networks have been broadly adopted to learning functions mapping between image data and various semantic descriptors, such as local object classes \citep{long2015fully}, or subjective quality \citep{bosse2017deep}. For example, \cite{lagunas2019similarity} present a method to learn similarities between materials based on their appearance and distinguish them in a feature space, informed by human perception. They describe the mappings between subjective perception and physical material parameters. This is a challenging task due to the impact of low-level properties, such as illumination and reflectance on the appearance of materials. The authors address this problem using deep features learned by a neural network trained on a bespoke dataset, annotated with around about one hundred classes of materials, captured under different conditions, including surface shape, illuminance and reflectance, expressed by environment maps and bidirectional reflectance distribution functions. In a subjective study, they encode materials in a perceptually informed feature space, outlining perceptual distance information relating to material pairs.\par

\subsection{Unsupervised and Semi-Supervised Alternatives}
\cite{schwartz2019recognizing} address the problem of material recognition from local visual information of materials to better model human interaction. They aim to reduce manual supervision in the process of encoding material characteristics, explaining visual attributes such as shiny or metallic and material properties that may not be visually or locally discoverable such as softness. They present a novel method for material recognition consisting of perceptually informed distances between materials and attribute spaces based on the distances.

Semi-supervised and unsupervised approaches have also been adopted in tackling such problems. For example, \cite{gaur2019superpixel} propose a novel deep learning architecture to cluster materials from a given dataset, improving state-of-the-art superpixel algorithms by combining segmentation of images into perceptually meaningful pixel clusters with a novel unsupervised clustering method based on superpixel embeddings. A novel loss function uses a variable margin that compensates for the limitations of classic superpixel algorithms in segmenting texture patterns, allowing the convolutional neural network to cluster superpixel labels based on their embeddings requiring no manual supervision or annotations.\par%

\cite{xia2017w} introduce a novel deep learning model for unsupervised image segmentation tasks. Their network is composed of an encoder and a decoder connected together to reconstruct an input image, producing a segmentation map, and distinguishing different materials depicted by the input image. 

\cite{kiechle2018model} present a novel method for segmenting textural patterns in input image data reducing the requirements for large-scale datasets representing exemplary features that the model trains to predict. Instead, they propose a framework that learns convolutional features from a small set of images or image patches. Their method shows competitive performance metrics against standard texture segmentation benchmarks, revealing the potential of this experimental method for material tagging. 

\subsection{Discussion}
Considering the broad field of computer vision and focusing on techniques that have a direct application to the retrieval of acoustic characteristics from \acrshortpl{ve} and assigning properties to environment geometry, the area is generally underdeveloped and has potential for improvements. Table~\ref{tab:material-recongition-techniques} summarises relevant experimental methods discussed.\par
A crucial finding within this area derives from \cite{schissler2017acoustic}, introducing some of the first approaches of scene understanding systems for sound rendering, projecting these into use cases for multi-modal AR and identifying limitations that future work should address to be around improved material recognition and inference on outdoor scenes. In general, the problem of recognising materials both in physical and virtual environments remains an open research question within these domains due to the challenging task of associating semantics to the visual appearance of surfaces in complex scenes, which depends on factors associated with the physical properties of surfaces or lighting conditions.\par 
Supervised methods, especially considering~\cite{kim2019immersive}'s work, can classify materials from their visual representation and provide input acoustic rendering pipelines. However, one drawback is the specificity of these methods to the acoustic materials expressed by data used to train the model.\par
Thanks to their abilities to handle large amounts of unlabelled data or a very small set of representative images, unsupervised methods have a lot of potential in addressing acoustic material tagging in \acrshortpl{ve}. This approach would require an additional step toward mapping the latent representation of clusters defined by the segmentation network to acoustic characteristics, matching the visual features learned by the feature extractor to their acoustic absorption or reflection characteristics. However, this shortcoming can become an advantage when artistic control is wanted, as existing acoustic materials within a complex scene could be re-mapped and controlled by their visual features.\par
Overall, \acrshortpl{cnn} are becoming optimised and fast enough that can be embedded in real-time systems, though generalising on a diverse set of surfaces and use cases is still an open research domain.
\input{material-recognition-techniques}

\section{Human Factors and Perceptual Rendering}\label{sec:perceptual-rendering}
\cite{bonneel2010bimodal}'s study investigates the influence of audio-visual stimuli, as well as the interaction of graphics and audio, on material perception. They designed an experiment testing whether graphics and audio have significant effects on the subjective perception of material qualities. The goal is to determine the minimum level of detail expressed by visual stimuli needed to evoke realism in observers, establishing a set of guidelines that can improve the performance of rendering techniques by culling and simplifying geometry maintaining significant perceptual responses.

\cite{Dolhasz_2020_CVPR}'s work around areas of perceptually-informed rendering expands the goal of investigating \acrshort{lod} threshold in image compositions, though the authors expand towards encoding perceptual responses into a latent space than automate the generation of perceptually-valid stimuli. The authors sample a large dataset of perceptual responses by prescribing a test to participants who were tasked with discriminating images with transformations from a given set. The goal of the authors is to fit a model on subjective responses that can then automate the suprathreshold detection process, which can be used within \acrshort{gan}-like models around automatic generation or transformations of content. This work can have a significant impact on multimodal rendering domains, as encoding perceptual responses can feed into sound rendering pipelines, enabling \acrshort{gan} to leverage discriminators learned on human perception.\par
Very recently around this area,~\cite{manocha2021cdpam} presented a perceptual similarity metric by encoding perceptual distances between audio signals. The authors use \acrshortpl{cnn} to train a model encoding subjective responses associated with pairs of audio signals, outputting a perceptual distance metric. The model is a great contribution to the field of audio quality evaluation as it can express \acrshortpl{jnd} between unseen pairs of audio signals, measuring perceptual distances or detecting transformations or perturbation audio data. In the domain of sound rendering, this model could optimise the laborious process of testing and sampling human perception to measure subjective factors of simulated auditory displays.\par

% \cite{slater2009visual} Realistic rendering affects presence and interactions

% \subsection{Findings and Limitations}
% Cross-modal interactions within VEs are generated from representations of entities that are consumed by rendering algorithms to produce audio-visual stimuli. 


\subsection{Perception of Audio Quality}
\cite{rummukainen2018audio} pioneered the field of audio quality evaluation in immersive technology by porting MUSHRA-like testing to \acrshort{vr} platforms, evaluating the impact of audio engines in interactive multi-modal VEs. The MUlti-Stimulus ranking test with Hidden Reference and Anchor (MUSHRA), described in the International Standard BS1534~\citep{liebetrau2014revision}, is a standard approach for evaluating the perceived audio quality of a system, often employed to evaluate coding, compression or processing tasks in the audio domain \citep{series2014method}. Thanks to~\cite{waet2015}'s web implementations, MUSHRA methods have been providing an essential tool for A/B comparisons or audio effects or algorithms and can be used to evaluate the quality of acoustic phenomena simulated with rendering techniques, such as reverberation, echo, diffraction or other soundscape characteristics that can be encoded in \acrshortpl{rir}.\par
\cite{rummukainen2018audio}'s framework gives MUSHRA additional dimensions by implementing the method as \acrshort{vr} scenarios, enabling the evaluation of renderer or spatialiser systems such as \acrshort{hrtf} spatialisers. The \acrshort{vr} nature of the framework can provide a wide breadth of metrics associated with the interaction between the listener, sound-emitting entities, the environment and tasks or procedures. The user study the authors conducted demonstrates how these metrics can provide further insights into perceptive aspects, for instance, showing how participants dwelled around testing areas during the execution of the procedure. With modern \acrshortpl{hmd} providing more and better interaction and sensing technology, researchers have access to eye, head, or hand-tracking data, as well as more information regarding scene elements of the \acrshort{ve}. Such data is generally unexplored, and investigation should explore how acoustic renderers affect subjective responses to audio stimuli.\par

\subsection{Psychoacoustic Characterisation of Sound Propagation Methods}
In light of the discussion on novel sound rendering pipelines in earlier sections (see Section~\ref{sec:sound-rendering-advances-discussion}), there is a rising need for profiling the psychoacoustic factors of simulated auditory stimuli. \cite{gonzalez2023binaural} present a toolbox providing an acoustic binaural rendering system, exposing parameters that researchers can measure for subjective evaluations. Their proposed framework provides control over listener pose information, sound source spatial information and binaural rendering parameters. In addition, they enable annotations on audio stimuli, allowing participants to save contextual information related to tasks administered. A point of expansion for this method may be the limited reverberation models available: the toolbox would benefit from interfacing with arbitrary sound propagation systems, allowing researchers to test recent experimental advances in sound rendering.\par
With the human listener as the central and final link in the chain of a sound rendering system, it is essential to consider how the audio display presented to the listener is affected by aspects related to human perception and psychoacoustic abilities performed by the \acrshort{has}. Due to the applications of sound rendering in \acrshortpl{ve} within serious and entertainment domains, researchers often base subjective evaluations of sound rendering techniques on task performance, studying how sound rendering techniques affect interactions, navigation, localisation or other activities influenced by the hearing sense.\par
\cite{mehra2015wave} presented a novel, wave-based sound rendering technique aimed at VR applications, advancing the domain of particle simulations for interactive sound rendering. One of the key contributions of their approach is providing a system offering realistic sound propagation between moving sound sources and listeners and can adapt to large, complex scenes. Their system offers spatial audio reproduction based on head tracking features of HMDs and position information of the listener in the \acrshort{ve}. The inherent limitation of their approach is the required pre-computation stage for evaluating acoustic energy transfers between geometry and objects in order to solve particle equations and generate the wave propagation field that can then be solved at runtime using general-purpose GPUs.\par
Here, the need for an evaluation of psychoacoustic factors arises for considering whether the perceived quality, subjective and psychoacoustic benefits outweigh the limitation of the pre-computation phase. They gathered 30 participants for their between-subjects experiment, 13 of whom had prior experience with VR technology, and the procedure they were asked to follow was the localisation of a sound-emitting object. The authors delegated a group for the navigation procedure using their renderer and a group using a geometrical acoustics renderer. They show that their wave-based sound renderer allowed a $27\%$ increase in localisation abilities in participants. Some of the limitations of this evaluation lie in the employment of an outdated image source-based renderer with edge diffraction as a comparison, altering the fairness of the study and the singular procedure used, as opposed to a range of different psychoacoustic-based activities that could be tested.\par

\cite{hacihabiboglu2017perceptual} discuss how perceptual aspects should influence the design of sound rendering pipelines. By reviewing a body of work around auralisation systems and sound propagation for interactive applications, they draw an effective pipeline design. Their design has the auralisation system revolving around a simplified model of the environment geometry and considering material properties, reverberation characteristics, and source directivity patterns as well as spatial information on the listener provided as input to the pipeline. Due to the complexity and computational requirements associated with the propagation algorithms and the rendering aspect of the system, the authors recommend perceptual culling to reduce the load and optimise the process for complex scenes with concurrent sound sources.\par

\cite{holographic_localisation} conducted the first investigation of psychoacoustic factors in \acrshort{ar}, where researchers tested how well holographic audio could be used to attract users' attention towards a given location. Effectively, the study represents a pioneering methodology towards the effectiveness of spatialised audio for psychoacoustic tasks, demonstrating the significance of sound rendering pipelines within the realm of interactions in \acrshort{ar} platforms.\par

\subsection{Findings and Limitations}
Recent work in multimodal rendering has highlighted the rising need for sampling human perception to measure and evaluate subjective factors in multimodal displays. A common denominator in rendering problems is the lack of data on perceptual responses obtained by simulated displays, allowing for dynamic changes in \acrshort{lod} and optimisation of computational resources. This problem becomes central in rendering pipelines that consider dynamic scenes with geometry being manipulated online or in the case of \acrshort{ar} platforms that work with reconstruction of real space surrounding the viewer. The dynamic nature of the platform often presents varying accuracy and precision in recognising and tracking real space and it is crucial to define the error margin in simulated displays before the user notices incoherence in stimuli.

\section{Conclusions}
Overall, the fields of sound rendering and sound propagation in immersive environments have advanced by significant strides into cutting-edge differentiable methods for simulating auditory stimuli. From approximating acoustic characteristics of a soundscape from a single photograph \citep{Singh_2021_ICCV} to generating thousands of \acrshortpl{ir} from a 3D representation of a given environment \citep{ratnarajah2022mesh2ir}, there is now a plethora of methods that can adapt to varying needs of realism and accuracy.

\begin{itemize}
    \item The rising development of computer vision methods is generating momentum towards novel sound propagation methods
    \item Novel sound propagation methods could potentially be feasible for AR platforms
    \item But we need to profile psychoacoustic factors of standard methods before venturing in experimental sound propagation in ar
    \item Even though GA methods are outdated, they have well-defined requirements and perceptual responses.
 
\end{itemize}

